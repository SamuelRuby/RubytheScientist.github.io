{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Manipulation with Pandas",
      "provenance": [],
      "authorship_tag": "ABX9TyMHWmdwYK0e7yLBuVt5trP7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelRuby/RubytheScientist.github.io/blob/main/Data_Manipulation_with_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5s9qSw5dJlx"
      },
      "outputs": [],
      "source": [
        "Data Manipulation with pandas \n",
        " Jun 27, 2020\n",
        "Base on DataCamp.\n",
        "\n",
        "DataFrames\n",
        "Introducing DataFrames\n",
        "Inspecting a DataFrame\n",
        ".head() returns the first few rows (the “head” of the DataFrame).\n",
        ".info() shows information on each of the columns, such as the data type and number of missing values.\n",
        ".shape returns the number of rows and columns of the DataFrame.\n",
        ".describe() calculates a few summary statistics for each column.\n",
        "# Print the head of the homelessness data\n",
        "print(homelessness.head())\n",
        "\n",
        "# Print information about homelessness\n",
        "print(homelessness.info())\n",
        "\n",
        "# Print the shape of homelessness\n",
        "print(homelessness.shape)\n",
        "\n",
        "# Print a description of homelessness\n",
        "print(homelessness.describe())\n",
        "Parts of a DataFrame\n",
        ".values: A two-dimensional NumPy array of values.\n",
        ".columns: An index of columns: the column names.\n",
        ".index: An index for the rows: either row numbers or row names.\n",
        "# Import pandas using the alias pd\n",
        "import pandas as pd\n",
        "\n",
        "# Print a 2D NumPy array of the values in homelessness.\n",
        "print(homelessness.values)\n",
        "\n",
        "# Print the column names of homelessness\n",
        "print(homelessness.columns)\n",
        "\n",
        "# Print the row index of homelessness\n",
        "print(homelessness.index)\n",
        "Sorting and subsetting\n",
        "Sorting rows\n",
        "# Sort homelessness by individual\n",
        "homelessness_ind = homelessness.sort_values('individuals')\n",
        "\n",
        "# Sort homelessness by descending family members\n",
        "homelessness_fam = homelessness.sort_values('family_members',ascending=False)\n",
        "\n",
        "# Sort homelessness by region, then descending family members\n",
        "homelessness_reg_fam = homelessness.sort_values(['region','family_members'], ascending = [True, False])\n",
        "Subsetting columns\n",
        "# Select the individuals column\n",
        "individuals = homelessness['individuals']\n",
        "\n",
        "# Select the state and family_members columns\n",
        "state_fam = homelessness[['state','family_members']]\n",
        "\n",
        "# Select only the individuals and state columns, in that order\n",
        "ind_state = homelessness[['individuals','state']]\n",
        "Subsetting rows\n",
        "# Filter for rows where individuals is greater than 10000\n",
        "ind_gt_10k = homelessness[homelessness['individuals']>10000]\n",
        "\n",
        "# Filter for rows where region is Mountain\n",
        "mountain_reg = homelessness[homelessness['region']==\"Mountain\"]\n",
        "\n",
        "# Filter for rows where family_members is less than 1000 \n",
        "# and region is Pacific\n",
        "fam_lt_1k_pac = homelessness[(homelessness['family_members']<1000) & (homelessness['region']==\"Pacific\")]\n",
        "Subsetting rows by categorical variables\n",
        "|, .isin()\n",
        "\n",
        "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
        "south_mid_atlantic = homelessness[(homelessness['region']==\"South Atlantic\") | (homelessness['region']==\"Mid-Atlantic\")]\n",
        "\n",
        "# The Mojave Desert states\n",
        "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
        "\n",
        "# Filter for rows in the Mojave Desert states\n",
        "mojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n",
        "New columns\n",
        "# Add total col as sum of individuals and family_members\n",
        "homelessness['total'] = homelessness['individuals'] + homelessness['family_members']\n",
        "\n",
        "# Add p_individuals col as proportion of individuals\n",
        "homelessness['p_individuals'] = homelessness['individuals'] / homelessness['total']\n",
        "\n",
        "# See the result\n",
        "print(homelessness)\n",
        "Combo-attack!\n",
        "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
        "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\n",
        "\n",
        "# Subset rows for indiv_per_10k greater than 20\n",
        "high_homelessness = homelessness[homelessness['indiv_per_10k']>20]\n",
        "\n",
        "# Sort high_homelessness by descending indiv_per_10k\n",
        "high_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending=False)\n",
        "\n",
        "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
        "result = high_homelessness_srt[['state','indiv_per_10k']]\n",
        "\n",
        "# See the result\n",
        "print(result)\n",
        "Aggregating Data\n",
        "Summary Statistics\n",
        "Mean and median\n",
        "# Print the head of the sales DataFrame\n",
        "print(sales.head())\n",
        "\n",
        "# Print the info about the sales DataFrame\n",
        "print(sales.info())\n",
        "\n",
        "# Print the mean of weekly_sales\n",
        "print(sales['weekly_sales'].mean())\n",
        "\n",
        "# Print the median of weekly_sales\n",
        "print(sales['weekly_sales'].median())\n",
        "Summarizing dates\n",
        "# Print the maximum of the date column\n",
        "print(sales['date'].max())\n",
        "> 2012-10-26 00:00:00\n",
        "\n",
        "# Print the minimum of the date column\n",
        "print(sales['date'].min())\n",
        "> 2010-02-05 00:00:00\n",
        "Efficient summaries\n",
        "The .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super efficient.\n",
        "\n",
        "# A custom IQR function\n",
        "def iqr(column):\n",
        "    return column.quantile(0.75) - column.quantile(0.25)\n",
        "    \n",
        "# Print IQR of the temperature_c column\n",
        "print(sales['temperature_c'].agg(iqr)\n",
        "\n",
        "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
        "print(sales[[\"temperature_c\", 'fuel_price_usd_per_l', 'unemployment']].agg(iqr))\n",
        "> temperature_c           16.583\n",
        "  fuel_price_usd_per_l     0.073\n",
        "  unemployment             0.565\n",
        "  dtype: float64\n",
        "\n",
        "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
        "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n",
        ">         temperature_c  fuel_price_usd_per_l  unemployment\n",
        "iqr            16.583                 0.073         0.565\n",
        "median         16.967                 0.743         8.099\n",
        "Cumulative statistics\n",
        ".cummax(),.cummin(), .cumprod()\n",
        "\n",
        "# Sort sales_1_1 by date\n",
        "sales_1_1 = sales_1_1.sort_values('date', ascending=True)\n",
        "\n",
        "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
        "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n",
        "\n",
        "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
        "sales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n",
        "\n",
        "# See the columns you calculated\n",
        "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n",
        "Counting\n",
        "Dropping duplicates\n",
        "# Drop duplicate store/type combinations\n",
        "store_types = sales.drop_duplicates(subset=['store','type'])\n",
        "print(store_types.head())\n",
        "\n",
        "# Drop duplicate store/department combinations\n",
        "store_depts = sales.drop_duplicates(subset=['store','department'])\n",
        "print(store_depts.head())\n",
        "\n",
        "# Subset the rows that are holiday weeks and drop duplicate dates\n",
        "holiday_dates = sales[sales['is_holiday']==True].drop_duplicates('date')\n",
        "\n",
        "# Print date col of holiday_dates\n",
        "print(holiday_dates['date'])\n",
        "Counting categorical variables\n",
        "# Count the number of stores of each type\n",
        "store_counts = store_types['type'].value_counts()\n",
        "print(store_counts)\n",
        "\n",
        "# Get the proportion of stores of each type\n",
        "store_props = store_types['type'].value_counts(normalize=True)\n",
        "print(store_props)\n",
        "\n",
        "# Count the number of each department number and sort\n",
        "dept_counts_sorted = store_depts['department'].value_counts(sort=True)\n",
        "print(dept_counts_sorted)\n",
        "\n",
        "# Get the proportion of departments of each number and sort\n",
        "dept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)\n",
        "print(dept_props_sorted)\n",
        "Grouped summary statistics\n",
        ".groupby()\n",
        "\n",
        "What percent of sales occurred at each store type?\n",
        "# Calc total weekly sales\n",
        "sales_all = sales[\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type A stores, calc total weekly sales\n",
        "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type B stores, calc total weekly sales\n",
        "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type C stores, calc total weekly sales\n",
        "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Get proportion for each type\n",
        "sales_propn_by_type = [sales_A, sales_B, sales_C] / (sales_A+sales_B+sales_C)\n",
        "print(sales_propn_by_type)\n",
        "> [0.9097747 0.0902253 0.       ]\n",
        "Calculations with .groupby()\n",
        "# Group by type; calc total weekly sales\n",
        "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
        "\n",
        "# Get proportion for each type\n",
        "sales_propn_by_type = sales_by_type/sum(sales.weekly_sales) \n",
        "print(sales_propn_by_type)\n",
        "> type\n",
        "  A    0.91\n",
        "  B    0.09\n",
        "  Name: weekly_sales, dtype: float64\n",
        "\n",
        "# Group by type and is_holiday; calc total weekly sales\n",
        "sales_by_type_is_holiday = sales.groupby([\"type\",'is_holiday'])[\"weekly_sales\"].sum()\n",
        "print(sales_by_type_is_holiday)\n",
        "> type  is_holiday\n",
        "  A     False         2.337e+08\n",
        "        True          2.360e+04\n",
        "  B     False         2.318e+07\n",
        "        True          1.621e+03\n",
        "  Name: weekly_sales, dtype: float64\n",
        "Multiple grouped summaries\n",
        "# Import NumPy with the alias np\n",
        "import numpy as np\n",
        "\n",
        "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
        "sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
        "\n",
        "# Print sales_stats\n",
        "print(sales_stats)\n",
        "\n",
        "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
        "unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
        "\n",
        "# Print unemp_fuel_stats\n",
        "print(unemp_fuel_stats)\n",
        "Pivot tables\n",
        "The .pivot_table() method is just an alternative to .groupby().\n",
        "\n",
        "Pivoting on one variable\n",
        "# Pivot for mean weekly_sales for each store type\n",
        "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')\n",
        "\n",
        "# Print mean_sales_by_type\n",
        "print(mean_sales_by_type)\n",
        ">       weekly_sales\n",
        "  type              \n",
        "  A        23674.667\n",
        "  B        25696.678\n",
        "\n",
        "# Pivot for mean and median weekly_sales for each store type\n",
        "mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index= 'type', aggfunc=[np.mean, np.median])\n",
        "\n",
        "# Print mean_med_sales_by_type\n",
        "print(mean_med_sales_by_type)\n",
        ">              mean       median\n",
        "       weekly_sales weekly_sales\n",
        "  type                          \n",
        "  A       23674.667     11943.92\n",
        "  B       25696.678     13336.08\n",
        "\n",
        "# Pivot for mean weekly_sales by store type and holiday \n",
        "mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales', index= 'type', columns='is_holiday')\n",
        "\n",
        "# Print mean_sales_by_type_holiday\n",
        "print(mean_sales_by_type_holiday)\n",
        "> is_holiday      False    True \n",
        "  type                          \n",
        "  A           23768.584  590.045\n",
        "  B           25751.981  810.705\n",
        "Fill in missing values and sum values with pivot tables\n",
        "The .pivot_table() method has several useful arguments, including fill_value and margins.\n",
        "\n",
        "fill_value replaces missing values with a real value (known as imputation).\n",
        "margins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.\n",
        "# Print mean weekly_sales by department and type; fill missing values with 0\n",
        "print(sales.pivot_table(values='weekly_sales', index='department', columns='type', fill_value=0))\n",
        "\n",
        "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0, margins=True))\n",
        "Slicing and indexing\n",
        ".set_index(), reset_index()\n",
        "\n",
        "Explicit indexes\n",
        "Setting & removing indexes\n",
        "# Look at temperatures\n",
        "print(temperatures)\n",
        "\n",
        "# Index temperatures by city\n",
        "temperatures_ind = temperatures.set_index('city')\n",
        "\n",
        "# Look at temperatures_ind\n",
        "print(temperatures_ind)\n",
        "\n",
        "# Reset the index, keeping its contents\n",
        "print(temperatures_ind.reset_index())\n",
        "\n",
        "# Reset the index, dropping its contents\n",
        "print(temperatures_ind.reset_index(drop=True))\n",
        "Slicing and subsetting with .loc and .iloc\n",
        "# Make a list of cities to subset on\n",
        "cities = ['Moscow', 'Saint Petersburg']\n",
        "\n",
        "# Subset temperatures using square brackets\n",
        "print(temperatures[temperatures['city'].isin(cities)])\n",
        "\n",
        "# Subset temperatures_ind using .loc[]\n",
        "print(temperatures_ind.loc[cities])\n",
        "Setting multi-level indexes\n",
        "# Index temperatures by country & city\n",
        "temperatures_ind = temperatures.set_index([\"country\",\"city\"])\n",
        "\n",
        "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
        "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"),(\"Pakistan\",\"Lahore\")]\n",
        "\n",
        "# Subset for rows to keep\n",
        "print(temperatures_ind.loc[rows_to_keep])\n",
        "Sorting by index values\n",
        "# Sort temperatures_ind by index values\n",
        "print(temperatures_ind.sort_index())\n",
        "\n",
        "# Sort temperatures_ind by index values at the city level\n",
        "print(temperatures_ind.sort_index(level='city'))\n",
        "\n",
        "# Sort temperatures_ind by country then descending city\n",
        "print(temperatures_ind.sort_index(level=['country','city'], ascending=[True, False]))\n",
        "Slicing and subsetting with .loc and .iloc\n",
        "Slicing index values\n",
        "Compared to slicing lists, there are a few things to remember.\n",
        "\n",
        "You can only slice an index if the index is sorted (using .sort_index()).\n",
        "To slice at the outer level, first and last can be strings.\n",
        "To slice at inner levels, first and last should be tuples.\n",
        "If you pass a single slice to .loc[], it will slice the rows.\n",
        "# Sort the index of temperatures_ind\n",
        "temperatures_srt = temperatures_ind.sort_index()\n",
        "\n",
        "# Subset rows from Pakistan to Russia\n",
        "print(temperatures_srt.loc['Pakistan':'Russia'])\n",
        "\n",
        "# Try to subset rows from Lahore to Moscow (This will return nonsense.)\n",
        "print(temperatures_srt.loc['Lahore':'Moscow'])\n",
        "\n",
        "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
        "print(temperatures_srt.loc[('Pakistan','Lahore'):('Russia', 'Moscow')])\n",
        "Slicing in both directions\n",
        "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
        "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad')])\n",
        "\n",
        "# Subset columns from date to avg_temp_c\n",
        "print(temperatures_srt.loc[:, 'date':'avg_temp_c'])\n",
        "\n",
        "# Subset in both directions at once\n",
        "# Subset columns from date to avg_temp_c\n",
        "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad'), 'date':'avg_temp_c'])\n",
        "Slicing time series\n",
        "Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, yyyy-mm-dd.\n",
        "\n",
        "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
        "temperatures_bool = temperatures[(temperatures[\"date\"] >= '2010-01-01') & (temperatures[\"date\"] <= '2011-12-31')]\n",
        "print(temperatures_bool)\n",
        "\n",
        "# Set date as an index\n",
        "temperatures_ind = temperatures.set_index('date')\n",
        "\n",
        "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
        "print(temperatures_ind.loc['2010':'2011'])\n",
        "\n",
        "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
        "print(temperatures_ind.loc['2010-08':'2011-2'])\n",
        "Subsetting by row/column number\n",
        "This is done using .iloc[], and like .loc[], it can take two arguments to let you subset by rows and columns.\n",
        "\n",
        "# Get 23rd row, 2nd column (index 22, 1)\n",
        "print(temperatures.iloc[22,2])\n",
        "\n",
        "# Use slicing to get the first 5 rows\n",
        "print(temperatures.iloc[0:5,:])\n",
        "\n",
        "# Use slicing to get columns 3 to 4\n",
        "print(temperatures.iloc[:,2:4])\n",
        "\n",
        "# Use slicing in both directions at once\n",
        "print(temperatures.iloc[0:5,2:4])\n",
        "Working with pivot tables\n",
        "Pivot temperature by city and year\n",
        "You can access the components of a date (year, month and day) using code of the form dataframe[\"column\"].dt.component. For example, the month component is dataframe[\"column\"].dt.month, and the year component is dataframe[\"column\"].dt.year.\n",
        "\n",
        "# Add a year column to temperatures\n",
        "temperatures['year'] = temperatures['date'].dt.year\n",
        "\n",
        "# Pivot avg_temp_c by country and city vs year\n",
        "temp_by_country_city_vs_year = temperatures.pivot_table('avg_temp_c', index=['country','city'], columns='year')\n",
        "\n",
        "# See the result\n",
        "print(temp_by_country_city_vs_year)\n",
        "Subsetting pivot tables\n",
        "A pivot table is just a DataFrame with sorted indexes. the .loc[] + slicing combination is often helpful.\n",
        "\n",
        "# Subset for Egypt to India\n",
        "temp_by_country_city_vs_year.loc['Egypt':'India']\n",
        "\n",
        "# Subset for Egypt, Cairo to India, Delhi\n",
        "temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi')]\n",
        "\n",
        "# Subset in both directions at once\n",
        "temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi'),'2005':'2010']\n",
        "Calculating on a pivot table\n",
        "# Get the worldwide mean temp by year\n",
        "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
        "\n",
        "# Filter for the year that had the highest mean temp\n",
        "print(mean_temp_by_year[mean_temp_by_year==mean_temp_by_year.max()])\n",
        "> year\n",
        "  2013    20.312\n",
        "  dtype: float64\n",
        "\n",
        "# Get the mean temp by city\n",
        "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n",
        "\n",
        "# Filter for the city that had the lowest mean temp\n",
        "print(mean_temp_by_city[mean_temp_by_city==mean_temp_by_city.min()])\n",
        "> country  city  \n",
        "  China    Harbin    4.877\n",
        "  dtype: float64\n",
        "Creating and Visualizing DataFrames\n",
        "Visualizing your data\n",
        "Which avocado size is most popular?\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Look at the first few rows of data\n",
        "print(avocados.head())\n",
        ">          date          type  year  avg_price   size    nb_sold\n",
        "    0  2015-12-27  conventional  2015       0.95  small  9.627e+06\n",
        "    1  2015-12-20  conventional  2015       0.98  small  8.710e+06\n",
        "    2  2015-12-13  conventional  2015       0.93  small  9.855e+06\n",
        "    3  2015-12-06  conventional  2015       0.89  small  9.405e+06\n",
        "    4  2015-11-29  conventional  2015       0.99  small  8.095e+06\n",
        "    \n",
        "# Get the total number of avocados sold of each size\n",
        "nb_sold_by_size = avocados.groupby('size')['nb_sold'].sum()\n",
        "\n",
        "# Create a bar plot of the number of avocados sold by size\n",
        "nb_sold_by_size.plot(kind='bar')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "Changes in sales over time\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the total number of avocados sold on each date\n",
        "nb_sold_by_date = avocados.groupby('date')['nb_sold'].sum()\n",
        "\n",
        "# Create a line plot of the number of avocados sold by date\n",
        "nb_sold_by_date.plot(kind='line')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "Avocado supply and demand\n",
        "# Scatter plot of nb_sold vs avg_price with title\n",
        "avocados.plot(x='nb_sold', y='avg_price', kind='scatter',title=\"Number of avocados sold vs. average price\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "Price of conventional vs. organic avocados\n",
        "# # Histogram of conventional avg_price \n",
        "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(bins=20, alpha=0.5)\n",
        "\n",
        "# Histogram of organic avg_price\n",
        "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(bins=20, alpha=0.5)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend([\"conventional\", \"organic\"])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "Missing values\n",
        "Finding missing values\n",
        ".isna(), .any()\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print a DataFrame that shows whether each value in avocados_2016 is missing or not.\n",
        "print(avocados_2016.isna())\n",
        "\n",
        "# Print a summary that shows whether any value in each column is missing or not.\n",
        "print(avocados_2016.isna().any())\n",
        "\n",
        "# Bar plot of missing values by variable\n",
        "avocados_2016.isna().sum().plot(kind='bar')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "Removing missing values\n",
        ".dropna()\n",
        "\n",
        "# Remove rows with missing values\n",
        "avocados_complete = avocados_2016.dropna()\n",
        "\n",
        "# Check if any columns contain missing values\n",
        "print(avocados_complete.isna().any())\n",
        "> date               False\n",
        "  avg_price          False\n",
        "  total_sold         False\n",
        "  small_sold         False\n",
        "  large_sold         False\n",
        "  xl_sold            False\n",
        "  total_bags_sold    False\n",
        "  small_bags_sold    False\n",
        "  large_bags_sold    False\n",
        "  xl_bags_sold       False\n",
        "  dtype: bool\n",
        "Replacing missing values\n",
        "# From previous step\n",
        "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
        "avocados_2016[cols_with_missing].hist()\n",
        "plt.show()\n",
        "\n",
        "# Fill in missing values with 0\n",
        "avocados_filled = avocados_2016.fillna(0)\n",
        "\n",
        "# Create histograms of the filled columns\n",
        "avocados_filled[cols_with_missing].hist()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "Creating DataFrames\n",
        "List of dictionaries\n",
        "# Create a list of dictionaries with new data\n",
        "avocados_list = [\n",
        "    {'date': \"2019-11-03\", 'small_sold': 10376832, 'large_sold': 7835071},\n",
        "    {'date': \"2019-11-10\", 'small_sold': 10717154, 'large_sold': 8561348},\n",
        "]\n",
        "\n",
        "# Convert list into DataFrame\n",
        "avocados_2019 = pd.DataFrame(avocados_list)\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(avocados_2019)\n",
        ">          date  small_sold  large_sold\n",
        "  0  2019-11-03    10376832     7835071\n",
        "  1  2019-11-10    10717154     8561348\n",
        "Dictionary of lists\n",
        "# Create a dictionary of lists with new data\n",
        "avocados_dict = {\n",
        "  \"date\": [\"2019-11-17\", \"2019-12-01\"],\n",
        "  \"small_sold\": [10859987, 9291631],\n",
        "  \"large_sold\": [7674135, 6238096]\n",
        "}\n",
        "\n",
        "# Convert dictionary into DataFrame\n",
        "avocados_2019 = pd.DataFrame(avocados_dict)\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(avocados_2019)\n",
        ">          date  small_sold  large_sold\n",
        "  0  2019-11-17    10859987     7674135\n",
        "  1  2019-12-01     9291631     6238096\n",
        "Reading and writing CSVs\n",
        "CSV to DataFrame\n",
        "# Read CSV as DataFrame called airline_bumping\n",
        "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
        "\n",
        "# Take a look at the DataFrame\n",
        "print(airline_bumping.head())\n",
        ">              airline  year  nb_bumped  total_passengers\n",
        "  0    DELTA AIR LINES  2017        679          99796155\n",
        "  1     VIRGIN AMERICA  2017        165           6090029\n",
        "  2    JETBLUE AIRWAYS  2017       1475          27255038\n",
        "  3    UNITED AIRLINES  2017       2067          70030765\n",
        "  4  HAWAIIAN AIRLINES  2017         92           8422734\n",
        "\n",
        "# For each airline, select nb_bumped and total_passengers and sum\n",
        "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
        "\n",
        "# Create new col, bumps_per_10k: no. of bumps per 10k passengers for each airline\n",
        "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
        "\n",
        "# Print airline_totals\n",
        "print(airline_totals)\n",
        "DataFrame to CSV\n",
        "# Create airline_totals_sorted\n",
        "airline_totals_sorted = airline_totals.sort_values('bumps_per_10k', ascending=False)\n",
        "\n",
        "# Print airline_totals_sorted\n",
        "print(airline_totals_sorted)\n",
        "\n",
        "# Save as airline_totals_sorted.csv\n",
        "airline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Data Merging Basics"
      ],
      "metadata": {
        "id": "cxVJa8KlZ6Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the licenses and biz_owners table on account\n",
        "licenses_owners = licenses.merge (biz_owners, on ='account')\n",
        "\n",
        "# Group the results by title then count the number of accounts\n",
        "counted_df = licenses_owners.groupby('title').agg({'account':'count'})\n",
        "\n",
        "# Sort the counted_df in desending order\n",
        "sorted_df = counted_df.sort_values(by = 'account', ascending = False)\n",
        "\n",
        "# Use .head() method to print the first few rows of sorted_df\n",
        "print(sorted_df.head())\n"
      ],
      "metadata": {
        "id": "PTWe5r6RZ8op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nkMP8RIFiZ-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the ridership, cal, and stations tables\n",
        "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
        "\t\t\t\t\t\t\t.merge(stations, on='station_id')\n",
        "\n",
        "# Create a filter to filter ridership_cal_stations\n",
        "filter_criteria = ((ridership_cal_stations['month'] == 7) \n",
        "                   & (ridership_cal_stations['day_type'] == 'Weekday') \n",
        "                   & (ridership_cal_stations['station_name'] == 'Wilson'))\n",
        "\n",
        "# Use .loc and the filter to select for rides\n",
        "print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())"
      ],
      "metadata": {
        "id": "LUN96fPJibWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ucVlritMSazu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROJECT"
      ],
      "metadata": {
        "id": "RjW1WRnlSbMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Github activity of the Scala Project\n"
      ],
      "metadata": {
        "id": "KeqRbcHXSc5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Loading in the data\n",
        "pulls_one = pd.read_csv('datasets/pulls_2011-2013.csv')\n",
        "pulls_two = pd.read_csv('datasets/pulls_2014-2018.csv')\n",
        "pull_files = pd.read_csv('datasets/pull_files.csv')\n",
        "\n",
        "#preparing and cleaning the data\n",
        "# Append pulls_one to pulls_two\n",
        "pulls = pulls_two.append(pulls_one)\n",
        "\n",
        "# Convert the date for the pulls object\n",
        "pulls['date'] = pd.to_datetime(pulls['date'],utc=True)\n",
        "\n",
        "#Merging the data frame\n",
        "# Merge the two DataFrames\n",
        "data = pulls.merge(pull_files,on='pid')\n",
        "\n",
        "#is the project still actively mainatained\n",
        "%matplotlib inline\n",
        "\n",
        "# Create a column that will store the month and the year, as a string\n",
        "pulls['month_year'] = pulls.apply(lambda x: str(x['date'].year) + '-' + str(x['date'].month), axis = 1)\n",
        "\n",
        "# Group by the month and year and count the pull requests\n",
        "counts = pulls.groupby('month_year').agg({'pid':'count'})\n",
        "\n",
        "# Plot the results\n",
        "counts.plot(kind='bar', figsize = (12,4))\n",
        "\n",
        "#is there still carmaderie in the project\n",
        "%matplotlib inline\n",
        "\n",
        "# Group by the submitter\n",
        "by_user = data.groupby('user').agg({'pid':'count'})\n",
        "print(by_user.head())\n",
        "# Plot the histogram\n",
        "by_user.plot(kind='hist', figsize = (12,4))\n",
        "\n",
        "#what files were changed in the last ten pull requests\n",
        "# Identify the last 10 pull requests\n",
        "last_10 = pulls.nlargest(10, 'pid', keep='last')\n",
        "\n",
        "# Join the two data sets\n",
        "joined_pr = last_10.merge(pull_files , on='pid')\n",
        "\n",
        "# Identify the unique files\n",
        "files = set(joined_pr['file'])\n",
        "\n",
        "# Print the results\n",
        "files\n",
        "\n",
        "#who made the most pull request to a given file\n",
        "# This is the file we are interested in:\n",
        "file = 'src/compiler/scala/reflect/reify/phases/Calculate.scala'\n",
        "\n",
        "# Identify the commits that changed the file\n",
        "file_pr = data[data['file'] == file]\n",
        "\n",
        "# Count the number of changes made by each developer\n",
        "author_counts = file_pr.groupby('user').count()\n",
        "\n",
        "# Print the top 3 developers\n",
        "author_counts.sort_values(by='pid', ascending=False).head(3)\n",
        "\n",
        "#who made the last 10 pull requests to a given file\n",
        "file = 'src/compiler/scala/reflect/reify/phases/Calculate.scala'\n",
        "\n",
        "# Select the pull requests that changed the target file\n",
        "file_pr = pull_files[pull_files.file == file]\n",
        "\n",
        "# Merge the obtained results with the pulls DataFrame\n",
        "joined_pr = file_pr.merge(pulls, on='pid')\n",
        "\n",
        "# Find the users of the last 10 most recent pull requests\n",
        "users_last_10 = set(joined_pr.nlargest(10, 'pid', keep='last')['user'])\n",
        "\n",
        "# Printing the results\n",
        "users_last_10\n",
        "\n",
        "\n",
        "#the pull requests of 2 special developers\n",
        "%matplotlib inline\n",
        "\n",
        "# The developers we are interested in\n",
        "authors = ['xeno-by', 'soc']\n",
        "\n",
        "# Get all the developers' pull requests\n",
        "by_author = pulls[pulls['user'].isin(authors)]\n",
        "\n",
        "# Count the number of pull requests submitted each year\n",
        "counts = by_author.groupby(['user', by_author['date'].dt.year]).agg({'pid': 'count'}).reset_index()\n",
        "\n",
        "# Convert the table to a wide format\n",
        "counts_wide = counts.pivot_table(index='date', columns='user', values='pid', fill_value=0)\n",
        "\n",
        "# Plot the results\n",
        "counts_wide.plot.bar()\n",
        "\n",
        "#visualising the contents of each developer\n",
        "uthors = ['xeno-by', 'soc']\n",
        "file = 'src/compiler/scala/reflect/reify/phases/Calculate.scala'\n",
        "\n",
        "# Select the pull requests submitted by the authors, from the `data` DataFrame\n",
        "by_author = data[(data.user == authors[0]) | (data.user == authors[1])]\n",
        "\n",
        "# Select the pull requests that affect the file\n",
        "by_file = by_author[by_author.file == file]\n",
        "\n",
        "# Group and count the number of PRs done by each user each year\n",
        "grouped = by_file.groupby(['user', by_file['date'].dt.year]).count()['pid'].reset_index()\n",
        "\n",
        "# Transform the data into a wide format\n",
        "by_file_wide = grouped.pivot_table(index='date', columns='user', values='pid', fill_value=0)\n",
        "\n",
        "# Plot the results\n",
        "by_file_wide.plot(kind='bar')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ouP-5XITSeFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}