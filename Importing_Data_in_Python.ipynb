{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Importing Data in Python ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfbIFQydcawUZOSW/AEmGY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelRuby/RubytheScientist.github.io/blob/main/Importing_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST44bBt_bchJ"
      },
      "outputs": [],
      "source": [
        "   \n",
        "Q1:-\n",
        "Open the file moby_dick.txt as read-only and store it in the variable file. Make sure to pass the filename enclosed in quotation marks ''.\n",
        "Print the contents of the file to the shell using the print() function. As Hugo showed in the video, you'll need to apply the method read() to the object file.\n",
        "Check whether the file is closed by executing print(file.closed).\n",
        "Close the file using the close() method.\n",
        "Check again that the file is closed as you did above.\n",
        "\n",
        "Solution:-\n",
        "# Open a file: file\n",
        "file = open(\"moby_dick.txt\",\"r\")\n",
        "\n",
        "# Print it\n",
        "print(file.read())\n",
        "\n",
        "# Check whether file is closed\n",
        "print(file.closed)\n",
        "\n",
        "# Close file\n",
        "file.close()\n",
        "\n",
        "# Check whether file is closed\n",
        "print(file.closed)\n",
        "\n",
        "Q2:-\n",
        "Open moby_dick.txt using the with context manager and the variable file.\n",
        "Print the first three lines of the file to the shell by using readline() three times within the context manager.\n",
        "\n",
        "Solution:-\n",
        "# Read & print the first 3 lines\n",
        "with open('moby_dick.txt') as file:\n",
        "    print(file.readline())\n",
        "    print(file.readline())\n",
        "    print(file.readline())\n",
        "\n",
        "Q3:-\n",
        "Fill in the arguments of np.loadtxt() by passing file and a comma ',' for the delimiter.\n",
        "Fill in the argument of print() to print the type of the object digits. Use the function type().\n",
        "Execute the rest of the code to visualize one of the rows of the data.\n",
        "\n",
        "Solution:-\n",
        "# Import package\n",
        "import numpy as np\n",
        "\n",
        "# Assign filename to variable: file\n",
        "file = 'digits.csv'\n",
        "\n",
        "# Load file as array: digits\n",
        "digits = np.loadtxt(file, delimiter=',')\n",
        "\n",
        "# Print datatype of digits\n",
        "print(type(digits))\n",
        "\n",
        "# Select and reshape a row\n",
        "im = digits[21, 1:]\n",
        "im_sq = np.reshape(im, (28, 28))\n",
        "\n",
        "# Plot reshaped data (matplotlib.pyplot already loaded as plt)\n",
        "plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
        "plt.show()\n",
        "\n",
        "Q4:-\n",
        "Complete the arguments of np.loadtxt(): the file you're importing is tab-delimited, you want to skip the first row and you only want to import the first and third columns.\n",
        "Complete the argument of the print() call in order to print the entire array that you just imported.\n",
        "\n",
        "Solution:-\n",
        "# Import numpy\n",
        "import numpy as np\n",
        "\n",
        "# Assign the filename: file\n",
        "file = 'digits_header.txt'\n",
        "\n",
        "# Load the data: data\n",
        "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0,2])\n",
        "\n",
        "# Print data\n",
        "print(data)\n",
        "\n",
        "Q5:-\n",
        "Complete the first call to np.loadtxt() by passing file as the first argument.\n",
        "Execute print(data[0]) to print the first element of data.\n",
        "Complete the second call to np.loadtxt(). The file you're importing is tab-delimited, the datatype is float, and you want to skip the first row.\n",
        "Print the 10th element of data_float by completing the print() command. Be guided by the previous print() call.\n",
        "Execute the rest of the code to visualize the data.\n",
        "\n",
        "Solution:-\n",
        "# Assign filename: file\n",
        "file = 'seaslug.txt'\n",
        "\n",
        "# Import file: data\n",
        "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
        "\n",
        "# Print the first element of data\n",
        "print(data[0])\n",
        "\n",
        "# Import data as floats and skip the first row: data_float\n",
        "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
        "\n",
        "# Print the 10th element of data_float\n",
        "print(data_float[9])\n",
        "\n",
        "# Plot a scatterplot of the data\n",
        "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
        "plt.xlabel('time (min.)')\n",
        "plt.ylabel('percentage of larvae')\n",
        "plt.show()\n",
        "\n",
        "Q6:-\n",
        "Import titanic.csv using the function np.recfromcsv() and assign it to the variable, d. You'll only need to pass file to it because it has the defaults delimiter=',' and names=True in addition to dtype=None!\n",
        "Run the remaining code to print the first three entries of the resulting array d.\n",
        "\n",
        "Solution:-\n",
        "# Assign the filename: file\n",
        "file = 'titanic.csv'\n",
        "\n",
        "# Import file using np.recfromcsv: d\n",
        "d = np.recfromcsv(file,delimiter=',',names=True,dtype=None)\n",
        "\n",
        "# Print out first three entries of d\n",
        "print(d[:3])\n",
        "\n",
        "Q7:-\n",
        "Import the pandas package using the alias pd.\n",
        "Read titanic.csv into a DataFrame called df. The file name is already stored in the file object.\n",
        "In a print() call, view the head of the DataFrame.\n",
        "\n",
        "Solution:-\n",
        "# Import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Assign the filename: file\n",
        "file = 'titanic.csv'\n",
        "\n",
        "# Read the file into a DataFrame: df\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# View the head of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "Q8:-\n",
        "Import the first 5 rows of the file into a DataFrame using the function pd.read_csv() and assign the result to data. You'll need to use the arguments nrows and header (there is no header in this file).\n",
        "Build a numpy array from the resulting DataFrame in data and assign to data_array.\n",
        "Execute print(type(data_array)) to print the datatype of data_array.\n",
        "\n",
        "Solution:-\n",
        "# Assign the filename: file\n",
        "file = 'digits.csv'\n",
        "\n",
        "# Read the first 5 rows of the file into a DataFrame: data\n",
        "data = pd.read_csv(file,nrows=5,header=None)\n",
        "\n",
        "# Build a numpy array from the DataFrame: data_array\n",
        "data_array = data.values\n",
        "\n",
        "# Print the datatype of data_array to the shell\n",
        "print(type(data_array))\n",
        "\n",
        "Q9:-\n",
        "Complete the sep (the pandas version of delim), comment and na_values arguments of pd.read_csv(). \n",
        "comment takes characters that comments occur after in the file, which in this case is '#'. na_values takes a list of strings to recognize as NA/NaN, \n",
        "in this case the string 'Nothing'.\n",
        "Execute the rest of the code to print the head of the resulting DataFrame and plot the histogram of the 'Age' of passengers aboard the Titanic.\n",
        "\n",
        "Solution:-\n",
        "# Import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assign filename: file\n",
        "file = 'titanic_corrupt.txt'\n",
        "\n",
        "# Import file: data\n",
        "data = pd.read_csv(file, sep='\\t', comment=\"#\", na_values=[\"Nothing\"])\n",
        "\n",
        "# Print the head of the DataFrame\n",
        "print(data.head())\n",
        "\n",
        "# Plot 'Age' variable in a histogram\n",
        "pd.DataFrame.hist(data[['Age']])\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "Introduction to other file types\n",
        "Excel spreadsheets MATLAB files\n",
        "SAS files\n",
        "Stata files\n",
        "HDF5 files\n",
        "Pickled files\n",
        "os libary\n",
        "# imports the library os\n",
        "import os \n",
        "\n",
        "#  stores the name of the current directory in a string called wd\n",
        "wd = os.getcwd()\n",
        "\n",
        "#  outputs the contents of the directory in a list to the shell\n",
        "os.listdir(wd)\n",
        "> ['titanic.txt', 'battledeath.xlsx']\n",
        "Loading a pickled file\n",
        "# Import pickle package\n",
        "import pickle \n",
        "\n",
        "# Open pickle file and load data: d\n",
        "with open('data.pkl', 'rb') as file: #  rb: it is read only for a binary file\n",
        "    d = pickle.load(file)\n",
        "\n",
        "# Print d\n",
        "print(d)\n",
        "> {'June': '69.4', 'Aug': '85', 'Airline': '8', 'Mar': '84.4'}\n",
        "\n",
        "# Print datatype of d\n",
        "print(type(d))\n",
        "> <class 'dict'>\n",
        "\n",
        "\n",
        "Excel files\n",
        "Listing sheets in Excel files\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Assign spreadsheet filename: file\n",
        "file = 'battledeath.xlsx'\n",
        "\n",
        "# Load spreadsheet: xls\n",
        "xls = pd.ExcelFile(file)\n",
        "\n",
        "# Print sheet names\n",
        "print(xls.sheet_names)\n",
        "> ['2002', '2004']\n",
        "Importing sheets from Excel files\n",
        "# Load the sheet '2004' into the DataFrame df1 using its name as a string.\n",
        "df1 = xls.parse('2004')\n",
        "\n",
        "# Print the head of the DataFrame df1\n",
        "print(df1.head())\n",
        "\n",
        "# Load the sheet 2002 into the DataFrame df2 using its index (0).\n",
        "df2 = xls.parse(0)\n",
        "\n",
        "# Print the head of the DataFrame df2\n",
        "print(df2.head())\n",
        "\n",
        "\n",
        "Customizing your spreadsheet import\n",
        "# Parse the first sheet by index. In doing so, skip the first row of data and name the columns 'Country' and 'AAM due to War (2002)' using the argument names. The values passed to skiprows and names all need to be of type list.\n",
        "df1 = xls.parse(0, skiprows=1, names=['Country','AAM due to War (2002)'])\n",
        "\n",
        "# Print the head of the DataFrame df1\n",
        "print(df1.head())\n",
        "\n",
        "# Parse the second sheet by index. In doing so, parse only the first column with the usecols parameter, skip the first row and rename the column 'Country'. The argument passed to usecols also needs to be of type list.\n",
        "df2 = xls.parse(1, usecols=[0], skiprows=1, names=['Country'])\n",
        "\n",
        "# Print the head of the DataFrame df2\n",
        "print(df2.head())\n",
        "\n",
        "\n",
        "Importing SAS files using pandas\n",
        "# Import the module SAS7BDAT from the library sas7bdat.\n",
        "from sas7bdat import SAS7BDAT\n",
        "\n",
        "# In the context of the file 'sales.sas7bdat', load its contents to a DataFrame df_sas, using the method to_data_frame() on the object file.\n",
        "with SAS7BDAT('sales.sas7bdat') as file:\n",
        "    df_sas = file.to_data_frame()\n",
        "\n",
        "# Print head of DataFrame\n",
        "print(df_sas.head())\n",
        "\n",
        "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
        "pd.DataFrame.hist(df_sas[['P']])\n",
        "plt.ylabel('count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "Importing Stata files using pandas\n",
        "Here, you'll gain expertise in importing Stata files as DataFrames using the pd.read_stata() function from pandas. The last exercise's file, 'disarea.dta', is still in your working directory.\n",
        "Use pd.read_stata() to load the file 'disarea.dta' into the DataFrame df.\n",
        "Print the head of the DataFrame df.\n",
        "Visualize your results by plotting a histogram of the column disa10. We’ve already provided this code for you, so just run it!\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load Stata file into a pandas DataFrame: df\n",
        "df = pd.read_stata('disarea.dta')\n",
        "\n",
        "# Print the head of the DataFrame df\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "HDF5 files\n",
        "Importing HDF5 files\n",
        "# Import packages\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# Assign filename: file\n",
        "file = 'LIGO_data.hdf5'\n",
        "\n",
        "# Load file: data\n",
        "data = h5py.File(file, 'r')\n",
        "\n",
        "# Print the datatype of the loaded file\n",
        "print(type(data))\n",
        "\n",
        "# Print the keys of the file\n",
        "for key in data.keys():\n",
        "    print(key)\n",
        "\n",
        "\n",
        "\n",
        "#Assign the HDF5 group data['strain'] to group.\n",
        "In the for loop, print out the keys of the HDF5 group in group.\n",
        "Assign to the variable strain the values of the time series data data['strain']['Strain'] using the attribute .value.\n",
        "Set num_samples equal to 10000, the number of time points we wish to sample.\n",
        "Execute the rest of the code to produce a plot of the time series data in LIGO_data.hdf5.\n",
        "Extracting data from your HDF5 file\n",
        "\n",
        "\n",
        "# Assign the HDF5 group data['strain'] to group\n",
        "group = data['strain']\n",
        "\n",
        "# Check out keys of group\n",
        "for key in group.keys():\n",
        "    print(key)\n",
        "\n",
        "# Assign to the variable strain the values of the time series data data['strain']['Strain'] using the attribute .value\n",
        "strain = data['strain']['Strain'].value\n",
        "\n",
        "# et num_samples equal to 10000, the number of time points we wish to sample.\n",
        "num_samples = 10000\n",
        "\n",
        "# Set time vector\n",
        "time = np.arange(0, 1, 1/num_samples)\n",
        "\n",
        "# Plot data\n",
        "plt.plot(time, strain[:num_samples])\n",
        "plt.xlabel('GPS Time (s)')\n",
        "plt.ylabel('strain')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "MATLAB files\n",
        "Loading .mat files\n",
        "In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it yields.\n",
        "\n",
        "The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at UC Davis. You can find the data and some great documentation here.\n",
        "\n",
        "Import the package scipy.io.\n",
        "Load the file 'albeck_gene_expression.mat' into the variable mat; do so using the function scipy.io.loadmat().\n",
        "Use the function type() to print the datatype of mat to the IPython shell.\n",
        "# Import the package scipy.io.\n",
        "import scipy.io\n",
        "\n",
        "# Load MATLAB file: mat\n",
        "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
        "\n",
        "# Print the datatype type of mat\n",
        "print(type(mat))\n",
        "> <class 'dict'>\n",
        "\n",
        "\n",
        "\n",
        "The structure of .mat in Python\n",
        "Use the method .keys() on the dictionary mat to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.\n",
        "Print the type of the value corresponding to the key 'CYratioCyt' in mat. Recall that mat['CYratioCyt'] accesses the value.\n",
        "Print the shape of the value corresponding to the key 'CYratioCyt' using the numpy function shape().\n",
        "Execute the entire script to see some oscillatory gene expression data!\n",
        "\n",
        "# Use the method .keys() on the dictionary mat to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.\n",
        "print(mat.keys())\n",
        "> > dict_keys(['__header__', '__version__', '__globals__', 'rfpCyt', 'rfpNuc', 'cfpNuc', 'cfpCyt', 'yfpNuc', 'yfpCyt', 'CYratioCyt'])\n",
        "\n",
        "# Print the type of the value corresponding to the key 'CYratioCyt' in mat. Recall that mat['CYratioCyt'] accesses the value.\n",
        "print(type(mat['CYratioCyt']))\n",
        "> <class 'numpy.ndarray'>\n",
        "\n",
        "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
        "print(np.shape(mat['CYratioCyt']))\n",
        "> (200, 137)\n",
        "\n",
        "\n",
        "\n",
        "Working with relational databases in Python\n",
        "\n",
        "Creating a database engine in Python\n",
        "# Import the function create_engine from the module sqlalchemy\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine\n",
        "engine = create_engine('sqlite:///Chinook.sqlite')\n",
        "\n",
        "# Using the method table_names() on the engine engine, assign the table names of 'Chinook.sqlite' to the variable table_names.\n",
        "table_names = engine.table_names()\n",
        "\n",
        "# Print the table names to the shell\n",
        "print(table_names)\n",
        "> ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n",
        "\n",
        "Querying relational databases in Python\n",
        "The Hello World of SQL Queries!\n",
        "# Import packages\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "# Create engine: engine\n",
        "engine = create_engine('sqlite:///Chinook.sqlite')\n",
        "\n",
        "# Open the engine connection as con using the method connect() on the engine.\n",
        "con = engine.connect()\n",
        "\n",
        "# Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
        "rs = con.execute('select * from Album')\n",
        "\n",
        "# Store all of your query results in the DataFrame df by applying the fetchall() method to the results rs.\n",
        "df = pd.DataFrame(rs.fetchall())\n",
        "\n",
        "# Close connection\n",
        "con.close()\n",
        "\n",
        "# Print head of DataFrame df\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Customizing the Hello World of SQL Queries\n",
        "# Perform query and save results to DataFrame: df\n",
        "with engine.connect() as con:\n",
        "    # Execute the SQL query that selects the columns LastName and Title from the Employee table. Store the results in the variable rs.\n",
        "    rs = con.execute(\"SELECT LastName, Title FROM Employee\")\n",
        "    # Apply the method fetchmany() to rs in order to retrieve 3 of the records. Store them in the DataFrame df. \n",
        "    df = pd.DataFrame(rs.fetchmany(size=3))\n",
        "    # Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.\n",
        "    df.columns = rs.keys()\n",
        "\n",
        "# Print the length of the DataFrame df\n",
        "print(len(df))\n",
        "> 3\n",
        "\n",
        "# Print the head of the DataFrame df\n",
        "print(df.head())\n",
        "\n",
        "#Filtering your database records using SQL's WHERE\n",
        "You can now execute a basic SQL query to select records from any table in your database and you can also perform simple query customizations to select particular columns and numbers of rows.\n",
        "\n",
        "There are a couple more standard SQL query chops that will aid you in your journey to becoming an SQL ninja.\n",
        "\n",
        "Let's say, for example that you wanted to get all records from the Customer table of the Chinook database for which the Country is 'Canada'. You can do this very easily in SQL using a SELECT statement followed by a WHERE clause as follows:\n",
        "\n",
        "SELECT * FROM Customer WHERE Country = 'Canada'\n",
        "In fact, you can filter any SELECT statement by any condition using a WHERE clause. This is called filtering your records.\n",
        "\n",
        "In this interactive exercise, you'll select all records of the Employee table for which 'EmployeeId' is greater than or equal to 6.\n",
        "\n",
        "Packages are already imported as follows:\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "Query away!\n",
        "\n",
        "Instructions\n",
        "Complete the argument of create_engine() so that the engine for the SQLite database 'Chinook.sqlite' is created.\n",
        "Execute the query that selects all records from the Employee table where 'EmployeeId' is greater than or equal to 6. Use the >= operator and assign the results to rs.\n",
        "Apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
        "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.\n",
        "\n",
        "# Create engine: engine\n",
        "engine = create_engine('sqlite:///Chinook.sqlite')\n",
        "\n",
        "# Open engine in context manager\n",
        "# Perform query and save results to DataFrame: df\n",
        "with engine.connect() as con:\n",
        "    rs = con.execute('SELECT * From Employee where EmployeeId >=6')\n",
        "    df = pd.DataFrame(rs.fetchall())\n",
        "    df.columns = rs.keys()\n",
        "\n",
        "# Print the head of the DataFrame df\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "Ordering your SQL records with ORDER BY\n",
        "\n",
        "# Open engine in context manager\n",
        "with engine.connect() as con:\n",
        "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")\n",
        "    df = pd.DataFrame(rs.fetchall())\n",
        "    df.columns = rs.keys()\n",
        "\n",
        "\n",
        "\n",
        "Querying relational databases directly with pandas\n",
        "Pandas and The Hello World of SQL Queries!\n",
        "# Import packages\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "# Create engine: engine\n",
        "engine = create_engine('sqlite:///Chinook.sqlite')\n",
        "\n",
        "# Execute query and store records in DataFrame: df\n",
        "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
        "\n",
        "# Print head of DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "# Open engine in context manager and store query result in df1\n",
        "with engine.connect() as con:\n",
        "    rs = con.execute(\"SELECT * FROM Album\")\n",
        "    df1 = pd.DataFrame(rs.fetchall())\n",
        "    df1.columns = rs.keys()\n",
        "\n",
        "# Confirm that both methods yield the same result\n",
        "print(df.equals(df1))\n",
        "> True\n",
        "\n",
        "\n",
        "\n",
        "Pandas for more complex querying\n",
        "# Import packages\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "# Create engine: engine\n",
        "engine = create_engine('sqlite:///Chinook.sqlite')\n",
        "\n",
        "# Execute query and store records in DataFrame: df\n",
        "df = pd.read_sql_query(\"SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate\", engine)\n",
        "\n",
        "# Print head of DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "Advanced querying: exploiting table relationships\n",
        "INNER JOIN\n",
        "# Open engine in context manager\n",
        "# Perform query and save results to DataFrame: df\n",
        "with engine.connect() as con:\n",
        "    # Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column of both.\n",
        "    rs = con.execute(\"SELECT Title,Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
        "    df = pd.DataFrame(rs.fetchall())\n",
        "    df.columns = rs.keys()\n",
        "\n",
        "# Filtering your INNER JOIN\n",
        "# Execute query and store records in DataFrame: df\n",
        "df = pd.read_sql_query(\"SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000\", engine)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Importing data from the Internet\n",
        "Importing files from the web\n",
        "Importing flat files\n",
        "# Import the function urlretrieve from the subpackage urllib.request\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Assign url of file: url\n",
        "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
        "\n",
        "# Opening and reading flat files from the web\n",
        "df = pd.read_csv(url, sep = ';')\n",
        "\n",
        "# Use the function urlretrieve() to save the file locally as 'winequality-red.csv'\n",
        "urlretrieve(url, 'winequality-red.csv')\n",
        "\n",
        "# Read file into a DataFrame and print its head\n",
        "df = pd.read_csv('winequality-red.csv', sep=';')\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Importing non-flat files\n",
        "# Import package\n",
        "import pandas as pd\n",
        "\n",
        "# Assign url of file: url\n",
        "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
        "\n",
        "# Read in all sheets of Excel file: xls\n",
        "xls = pd.read_excel(url, sheet_name=None) #  in order to import all sheets you need to pass None to the argument sheet_name\n",
        "\n",
        "# Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls.\n",
        "print(xls.keys())\n",
        "\n",
        "# Print the head of the first sheet (using its name, NOT its index),The sheet name is '1700'.\n",
        "print(xls['1700'].head())\n",
        "\n",
        "\n",
        "\n",
        "HTTP requests to import files from the web\n",
        "# Import the functions urlopen and Request from the subpackage urllib.request.\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "# Specify the url\n",
        "url = \"http://www.datacamp.com/teach/documentation\"\n",
        "\n",
        "# This packages the request: request\n",
        "request = Request(url)\n",
        "\n",
        "# Sends the request and catches the response: response\n",
        "response = urlopen(request)\n",
        "\n",
        "# Print the datatype of response\n",
        "print(type(response))\n",
        "> <class 'http.client.HTTPResponse'>\n",
        "\n",
        "# Extract the response: html\n",
        "html = response.read()\n",
        "\n",
        "# Print the html request results\n",
        "print(html)\n",
        "\n",
        "# Be polite and close the response!\n",
        "response.close()\n",
        "Performing HTTP requests in Python using package requests\n",
        "\n",
        "# Import the package requests.\n",
        "import requests\n",
        "\n",
        "# Specify the url: url\n",
        "url = \"http://www.datacamp.com/teach/documentation\"\n",
        "\n",
        "# Packages the request, send the request and catch the response: r\n",
        "r = requests.get(url)\n",
        "\n",
        "# Extract the response: text. Return the HTML of the webpage as a string; \n",
        "text = r.text\n",
        "\n",
        "# Print the html\n",
        "print(text)\n",
        "\n",
        "\n",
        "Scraping the web in Python\n",
        "Parsing HTML with BeautifulSoup\n",
        "# Import the function BeautifulSoup from the package bs4.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify url: url\n",
        "url = 'https://www.python.org/~guido/'\n",
        "\n",
        "# Package the request, send the request and catch the response: r\n",
        "r = requests.get(url)\n",
        "\n",
        "# Extracts the response as html: html_doc\n",
        "html_doc = r.text\n",
        "\n",
        "# Create a BeautifulSoup object from the HTML: soup\n",
        "soup = BeautifulSoup(html_doc)\n",
        "\n",
        "# Prettify the BeautifulSoup object: pretty_soup\n",
        "pretty_soup = soup.prettify()\n",
        "\n",
        "# Print the response\n",
        "print(pretty_soup)\n",
        "\n",
        "\n",
        "# Getting the text\n",
        "# Get the title of Guido's webpage: guido_title\n",
        "guido_title = soup.title\n",
        "\n",
        "# Print the title of Guido's webpage to the shell\n",
        "print(guido_title)\n",
        "\n",
        "# Get Guido's text: guido_text\n",
        "guido_text = soup.get_text()\n",
        "\n",
        "# Print Guido's text to the shell\n",
        "print(guido_text)\n",
        "\n",
        "\n",
        "# getting the hyperlinks\n",
        "# Find all 'a' tags (which define hyperlinks): a_tags\n",
        "a_tags = soup.find_all('a') # hyperlinks are defined by the HTML tag <a> but passed to find_all() without angle brackets;\n",
        "\n",
        "# Print the URLs to the shell\n",
        "# The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; to do this, for every element link in a_tags, you want to print() link.get('href').\n",
        "for link in a_tags:\n",
        "    print(link.get('href'))\n",
        "Interacting with APIs to import data from the web\n",
        "Loading and exploring a JSON\n",
        "# use the function json.load() within the context manager.\n",
        "with open(\"a_movie.json\") as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "# Use a for loop to print all key-value pairs in the dictionary json_data\n",
        "for k in json_data.keys():\n",
        "    print(k + ': ', json_data[k])\n",
        "\n",
        "\n",
        "import json\n",
        "with open(\"a_movie.json\") as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "json_data['Title']\n",
        "> 'The Social Network'\n",
        "json_data['Year']\n",
        "> '2010'\n",
        "\n",
        "\n",
        "\n",
        "APIs and interacting with the world wide web\n",
        "API requests\n",
        "# Import requests package\n",
        "import requests\n",
        "\n",
        "# Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network\n",
        "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
        "\n",
        "# Package the request, send the request and catch the response: r\n",
        "r = requests.get(url)\n",
        "\n",
        "# Print the text of the response\n",
        "print(r.text)\n",
        "\n",
        "\n",
        "# JSON–from the web to Python\n",
        "# Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.\n",
        "json_data = r.json()\n",
        "\n",
        "# Print each key-value pair in json_data\n",
        "for k in json_data.keys():\n",
        "    print(k + ': ', json_data[k])\n",
        "Checking out the Wikipedia API\n",
        "# Import package\n",
        "import requests\n",
        "\n",
        "# Assign URL to variable: url\n",
        "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
        "\n",
        "\n",
        "# Package the request, send the request and catch the response: r\n",
        "r = requests.get(url)\n",
        "\n",
        "# Decode the JSON data into a dictionary: json_data\n",
        "json_data = r.json()\n",
        "\n",
        "# Print the Wikipedia page extract\n",
        "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
        "print(pizza_extract)\n",
        "Diving deep into the Twitter API\n",
        "API Authentication\n",
        "The package tweepy is great at handling all the Twitter API OAuth Authentication details for you. All you need to do is pass it your authentication credentials.\n",
        "\n",
        "# Import the package tweepy\n",
        "import tweepy\n",
        "\n",
        "# Store OAuth authentication credentials in relevant variables\n",
        "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
        "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
        "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
        "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
        "\n",
        "# Pass OAuth details to tweepy's OAuth handler. Pass the parameters consumer_key and consumer_secret to the function tweepy.OAuthHandler().\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "# Complete the passing of OAuth credentials to the OAuth handler auth by applying to it the method set_access_token(), along with arguments access_token and access_token_secret.\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "Defined the tweet stream listener class\n",
        "class MyStreamListener(tweepy.StreamListener):\n",
        "    def __init__(self, api=None):\n",
        "        super(MyStreamListener, self).__init__()\n",
        "        self.num_tweets = 0\n",
        "        self.file = open(\"tweets.txt\", \"w\")\n",
        "\n",
        "    def on_status(self, status):\n",
        "        tweet = status._json\n",
        "        self.file.write( json.dumps(tweet) + '\\n' )\n",
        "        self.num_tweets += 1\n",
        "        if self.num_tweets < 100:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "        self.file.close()\n",
        "\n",
        "    def on_error(self, status):\n",
        "        print(status)\n",
        "\n",
        "\n",
        "Streaming tweets\n",
        "# Initialize Stream listener\n",
        "l = MyStreamListener()\n",
        "\n",
        "# Create your Stream object with authentication\n",
        "stream = tweepy.Stream(auth, l)\n",
        "\n",
        "# Filter Twitter Streams to capture data by the keywords:\n",
        "stream.filter(['clinton','trump','sanders','cruz'])\n",
        "Load and explore your Twitter data\n",
        "# Import package\n",
        "import json\n",
        "\n",
        "# String of path to file: tweets_data_path\n",
        "tweets_data_path = 'tweets.txt'\n",
        "\n",
        "# Initialize empty list to store tweets: tweets_data\n",
        "tweets_data = []\n",
        "\n",
        "# Open connection to file\n",
        "tweets_file = open(tweets_data_path, \"r\")\n",
        "\n",
        "# Read in tweets and store in list: tweets_data\n",
        "for line in tweets_file:\n",
        "    tweet = json.loads(line) #  load each tweet into a variable, tweet, using json.loads()\n",
        "    tweets_data.append(tweet) # ppend tweet to tweets_data\n",
        "\n",
        "# Close connection to file\n",
        "tweets_file.close()\n",
        "\n",
        "# Print the keys of the first tweet dict\n",
        "print(tweets_data[0].keys())\n",
        "\n",
        "\n",
        "Twitter data to DataFrame\n",
        "# Import package\n",
        "import pandas as pd\n",
        "\n",
        "# Build DataFrame of tweet texts and languages. Use pd.DataFrame() to construct a DataFrame of tweet texts and languages; to do so, the first argument should be tweets_data, a list of dictionaries. The second argument to pd.DataFrame() is a list of the keys you wish to have as columns\n",
        "df = pd.DataFrame(tweets_data, columns=['text','lang'])\n",
        "\n",
        "# Print head of DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "A little bit of Twitter text analysis\n",
        "Now that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the words 'clinton', 'trump', 'sanders' and 'cruz'. In the pre-exercise code, we have defined the following function word_in_text(), which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).\n",
        "\n",
        "Defined the following function word_in_text(), which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).\n",
        "\n",
        "import re\n",
        "\n",
        "def word_in_text(word, text):\n",
        "    word = word.lower()\n",
        "    text = text.lower()\n",
        "    match = re.search(word, text)\n",
        "\n",
        "    if match:\n",
        "        return True\n",
        "    return False\n",
        "Iterate over the rows of the DataFrame and calculate how many tweets contain each of our keywords. The list of objects for each candidate has been initialized to 0.\n",
        "\n",
        "# Initialize list to store tweet counts\n",
        "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
        "\n",
        "# Iterate through df, counting the number of tweets in which\n",
        "# each candidate is mentioned\n",
        "for index, row in df.iterrows():\n",
        "    clinton += word_in_text('clinton', row['text']) #  increases the value of clinton by 1 each time a tweet (text row) mentioning 'Clinton' is encountered; \n",
        "    trump += word_in_text('trump', row['text'])\n",
        "    sanders += word_in_text('sanders', row['text'])\n",
        "    cruz += word_in_text('cruz', row['text'])\n",
        "\n",
        "\n",
        "Plotting your Twitter data\n",
        "# Import packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set seaborn style\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "# Create a list of labels:cd\n",
        "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
        "\n",
        "# Plot the bar chart\n",
        "ax = sns.barplot(cd, [clinton, trump, sanders, cruz]) # The first argument should be the list of labels to appear on the x-axis (created in the previous step). The second argument should be a list of the variables you wish to plot(i.e. a list containing clinton, trump, etc).\n",
        "ax.set(ylabel=\"count\")\n",
        "plt.show()"
      ]
    }
  ]
}