{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pandas",
      "provenance": [],
      "authorship_tag": "ABX9TyNGFBYApYmJuB9F/gERLHYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelRuby/RubytheScientist.github.io/blob/main/Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3LrRhXa8CRe"
      },
      "outputs": [],
      "source": [
        "# Read in dataset\n",
        "import pandas as pd\n",
        "apps_with_duplicates = pd.read_csv(\"datasets/apps.csv\")\n",
        "\n",
        "# Drop duplicates from apps_with_duplicates\n",
        "apps = apps_with_duplicates.drop_duplicates()\n",
        "\n",
        "# Print the total number of apps\n",
        "print('Total number of apps in the dataset = ', len(apps))\n",
        "\n",
        "# Have a look at a random sample of 5 rows\n",
        "n = 5\n",
        "apps.sample(n)\n",
        "\n",
        "\n",
        "#Data cleaning\n",
        "# List of characters to remove\n",
        "chars_to_remove = ['+', ',', '$']\n",
        "# List of column names to clean\n",
        "cols_to_clean = ['Installs', 'Price']\n",
        "\n",
        "# Loop for each column in cols_to_clean\n",
        "for col in cols_to_clean:\n",
        "    # Loop for each char in chars_to_remove\n",
        "    for char in chars_to_remove:\n",
        "        # Replace the character with an empty string\n",
        "        apps[col] = apps[col].apply(lambda x: x.replace(char, ''))  \n",
        "\n",
        "# Print a summary of the apps dataframe\n",
        "print(apps.info())\n",
        "\n",
        "\n",
        "#Correcting Data types\n",
        "import numpy as np\n",
        "\n",
        "# Convert Installs to float data type\n",
        "apps['Installs'] = apps['Installs'].astype(float)\n",
        "\n",
        "# Convert Price to float data type\n",
        "apps['Price'] = apps['Price'].astype(float)\n",
        "\n",
        "# Checking dtypes of the apps dataframe\n",
        "print(apps.dtypes)\n",
        "\n",
        "\n",
        "#Exploring app categories\n",
        "import plotly\n",
        "plotly.offline.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "# Print the total number of unique categories\n",
        "num_categories = len(apps['Category'].unique())\n",
        "print('Number of categories = ', num_categories)\n",
        "\n",
        "# Count the number of apps in each 'Category'. \n",
        "num_apps_in_category = apps['Category'].value_counts()\n",
        "\n",
        "# Sort num_apps_in_category in descending order based on the count of apps in each category\n",
        "sorted_num_apps_in_category = num_apps_in_category.sort_values(ascending = False)\n",
        "\n",
        "data = [go.Bar(\n",
        "        x = num_apps_in_category.index, # index = category name\n",
        "        y = num_apps_in_category.values, # value = count\n",
        ")]\n",
        "\n",
        "plotly.offline.iplot(data)\n",
        "\n",
        "\n",
        "#Distribution of app rating\n",
        "# Average rating of apps\n",
        "avg_app_rating = apps['Rating'].mean()\n",
        "print('Average app rating = ', avg_app_rating)\n",
        "\n",
        "# Distribution of apps according to their ratings\n",
        "data = [go.Histogram(\n",
        "        x = apps['Rating']\n",
        ")]\n",
        "\n",
        "# Vertical dashed line to indicate the average app rating\n",
        "layout = {'shapes': [{\n",
        "              'type' :'line',\n",
        "              'x0': avg_app_rating,\n",
        "              'y0': 0,\n",
        "              'x1': avg_app_rating,\n",
        "              'y1': 1000,\n",
        "              'line': { 'dash': 'dashdot'}\n",
        "          }]\n",
        "          }\n",
        "\n",
        "plotly.offline.iplot({'data': data, 'layout': layout})\n",
        "\n",
        "\n",
        "#size and price of an app\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Select rows where both 'Rating' and 'Size' values are present (ie. the two values are not null)\n",
        "apps_with_size_and_rating_present = apps[(~apps['Rating'].isnull()) & (~apps['Size'].isnull())]\n",
        "\n",
        "# Subset for categories with at least 250 apps\n",
        "large_categories = apps_with_size_and_rating_present.groupby(['Category']).filter(lambda x: len(x) >= 250)\n",
        "\n",
        "# Plot size vs. rating\n",
        "plt1 = sns.jointplot(x = large_categories['Size'], y = large_categories['Rating'])\n",
        "\n",
        "# Select apps whose 'Type' is 'Paid'\n",
        "paid_apps = apps_with_size_and_rating_present[apps_with_size_and_rating_present['Type'] == 'Paid']\n",
        "\n",
        "# Plot price vs. rating\n",
        "plt2 = sns.jointplot(x = paid_apps['Price'], y = paid_apps['Rating'])\n",
        "\n",
        "\n",
        "#Relationship between app category and price \n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(15, 8)\n",
        "\n",
        "# Select a few popular app categories\n",
        "popular_app_cats = apps[apps.Category.isin(['GAME', 'FAMILY', 'PHOTOGRAPHY',\n",
        "                                            'MEDICAL', 'TOOLS', 'FINANCE',\n",
        "                                            'LIFESTYLE','BUSINESS'])]\n",
        "\n",
        "# Examine the price trend by plotting Price vs Category\n",
        "ax = sns.stripplot(x = popular_app_cats['Price'], y = popular_app_cats['Category'], jitter=True, linewidth=1)\n",
        "ax.set_title('App pricing trend across categories')\n",
        "\n",
        "# Apps whose Price is greater than 200\n",
        "apps_above_200 = popular_app_cats[popular_app_cats['Price'] > 200]\n",
        "apps_above_200[['Category', 'App', 'Price']]\n",
        "\n",
        "\n",
        "\n",
        "#filter out junk apps\n",
        "# Select apps priced below $100\n",
        "apps_under_100 = popular_app_cats[popular_app_cats['Price'] < 100]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(15, 8)\n",
        "\n",
        "# Examine price vs category with the authentic apps (apps_under_100)\n",
        "ax = sns.stripplot(x = 'Price', y = 'Category', data = apps_under_100, jitter = True, linewidth = 1)\n",
        "ax.set_title('App pricing trend across categories after filtering for junk apps')\n",
        "\n",
        "\n",
        "#popularity of paid apps vs free apps\n",
        "trace0 = go.Box(\n",
        "    # Data for paid apps\n",
        "    y = apps[apps['Type'] == 'Paid']['Installs'],\n",
        "    name = 'Paid'\n",
        ")\n",
        "\n",
        "trace1 = go.Box(\n",
        "    # Data for free apps\n",
        "    y = apps[apps['Type'] == 'Free']['Installs'],\n",
        "    name = 'Free'\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    title = \"Number of downloads of paid apps vs. free apps\",\n",
        "    yaxis = dict(title = \"Log number of downloads\",\n",
        "                type = 'log',\n",
        "                autorange = True)\n",
        ")\n",
        "\n",
        "# Add trace0 and trace1 to a list\n",
        "data = [trace0, trace1]\n",
        "plotly.offline.iplot({'data': data, 'layout': layout})\n",
        "\n",
        "#sentiment analysis of users\n",
        "# Load user_reviews.csv\n",
        "reviews_df = pd.read_csv('datasets/user_reviews.csv')\n",
        "\n",
        "# Join the two dataframes\n",
        "merged_df = pd.merge(apps, reviews_df, on = \"App\")\n",
        "\n",
        "# Drop NA values from Sentiment and Review columns\n",
        "merged_df = merged_df.dropna(subset = ['Sentiment', 'Review'])\n",
        "\n",
        "sns.set_style('ticks')\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(11, 8)\n",
        "\n",
        "# User review sentiment polarity for paid vs. free apps\n",
        "ax = sns.boxplot(x = 'Type', y = 'Sentiment_Polarity', data = merged_df)\n",
        "ax.set_title('Sentiment Polarity Distribution')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandas inner join"
      ],
      "metadata": {
        "id": "DVoWilKmb3AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge action_movies to the scifi_movies with right join\n",
        "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
        "                                   suffixes=('_act','_sci'))\n",
        "\n",
        "# From action_scifi, select only the rows where the genre_act column is null\n",
        "scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n",
        "\n",
        "# Merge the movies and scifi_only tables with an inner join\n",
        "movies_and_scifi_only = movies.merge(scifi_only, how='inner',\n",
        "                                     left_on='id', right_on='movie_id')\n",
        "\n",
        "# Print the first few rows and shape of movies_and_scifi_only\n",
        "print(movies_and_scifi_only.head())\n",
        "print(movies_and_scifi_only.shape)\n",
        "\n",
        "POPULAR GENRES WITH RIGHT JOIN\n",
        "# Use right join to merge the movie_to_genres and pop_movies tables\n",
        "genres_movies = movie_to_genres.merge(pop_movies, how='right', \n",
        "                                      left_on='movie_id', \n",
        "                                      right_on='id')\n",
        "\n",
        "# Count the number of genres\n",
        "genre_count = genres_movies.groupby('genre').agg({'id':'count'})\n",
        "\n",
        "# Plot a bar chart of the genre_count\n",
        "genre_count.plot(kind='bar')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CwhTDCW3b5Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequels in pandas\n"
      ],
      "metadata": {
        "id": "PgHH9pbJnh8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge sequels and financials on index id\n",
        "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
        "\n",
        "# Self merge with suffixes as inner join with left on sequel and right on id\n",
        "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n",
        "                             right_on='id', right_index=True,\n",
        "                             suffixes=('_org','_seq'))\n",
        "\n",
        "# Add calculation to subtract revenue_org from revenue_seq \n",
        "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
        "\n",
        "# Select the title_org, title_seq, and diff \n",
        "titles_diff = orig_seq[['title_org','title_seq','diff']]\n",
        "\n",
        "# Print the first rows of the sorted titles_diff\n",
        "print(titles_diff.sort_values('diff', ascending=False).head())\n",
        "\n"
      ],
      "metadata": {
        "id": "1lyFM5xdnlc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering Joins"
      ],
      "metadata": {
        "id": "4V704PabEafu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge employees and top_cust\n",
        "empl_cust = employees.merge(top_cust, on='srid', \n",
        "                                 how='left', indicator=True)\n",
        "\n",
        "# Select the srid column where _merge is left_only\n",
        "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
        "\n",
        "# Get employees not working with top customers\n",
        "print(employees[employees['srid'].isin(srid_list)])\n",
        "\n",
        "\n",
        "\n",
        "# Merge the non_mus_tck and top_invoices tables on tid\n",
        "tracks_invoices = non_mus_tcks.merge(top_invoices, on='tid', how='inner')\n",
        "\n",
        "# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\n",
        "top_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices['tid'])]\n",
        "\n",
        "# Group the top_tracks by gid and count the tid rows\n",
        "cnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':'count'})\n",
        "\n",
        "# Merge the genres table to cnt_by_gid on gid and print\n",
        "print(cnt_by_gid.merge(genres, on='gid'))\n"
      ],
      "metadata": {
        "id": "bgR-F5JeEcis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using the append method"
      ],
      "metadata": {
        "id": "yctg4QExH2RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the .append() method to combine the tracks tables\n",
        "metallica_tracks = tracks_ride.append([tracks_master, tracks_st], sort=False)\n",
        "\n",
        "# Merge metallica_tracks and invoice_items\n",
        "tracks_invoices = metallica_tracks.merge(invoice_items, on='tid', how='inner')\n",
        "\n",
        "# For each tid and name sum the quantity sold\n",
        "tracks_sold = tracks_invoices.groupby(['tid','name']).agg({'quantity':'sum'})\n",
        "\n",
        "# Sort in decending order by quantity and print the results\n",
        "print(tracks_sold.sort_values(by=['quantity'], ascending=False))"
      ],
      "metadata": {
        "id": "W5qgtmn-H4qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "merge.ordered"
      ],
      "metadata": {
        "id": "EYz8tueGL-ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
        "inflation_unemploy = pd.merge_ordered(inflation, unemployment, on='date', how='inner')\n",
        "\n",
        "# Print inflation_unemploy \n",
        "print(inflation_unemploy)\n",
        "\n",
        "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
        "inflation_unemploy.plot(x='unemployment_rate', y='cpi' , kind='scatter')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6tMOqrDML__m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "correlation"
      ],
      "metadata": {
        "id": "6yokLR_PMDvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use merge_ordered() to merge gdp and sp500, interpolate missing value\n",
        "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
        "                             how='left',  fill_method='ffill')\n",
        "\n",
        "# Subset the gdp and returns columns\n",
        "gdp_returns = gdp_sp500[['gdp', 'returns']]\n",
        "\n",
        "# Print gdp_returns correlation\n",
        "print (gdp_returns.corr())"
      ],
      "metadata": {
        "id": "JuWnC-PjME1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "using merge.asof to study stocks"
      ],
      "metadata": {
        "id": "SvVYHa7SMuNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use merge_asof() to merge jpm and wells\n",
        "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', suffixes=('', '_wells'), direction='nearest')\n",
        "\n",
        "\n",
        "# Use merge_asof() to merge jpm_wells and bac\n",
        "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on=['date_time'], suffixes=('_jpm', '_bac'), direction='nearest')\n",
        "\n",
        "\n",
        "# Compute price diff\n",
        "price_diffs = jpm_wells_bac.diff()\n",
        "\n",
        "# Plot the price diff of the close of jpm, wells and bac only\n",
        "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "npENMTMkMzUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use merg.asof to create dataset"
      ],
      "metadata": {
        "id": "xJMyRA9rNArz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge gdp and recession on date using merge_asof()\n",
        "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
        "\n",
        "# Create a list based on the row value of gdp_recession['econ_status']\n",
        "is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
        "\n",
        "# Plot a bar chart of gdp_recession\n",
        "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bbUouTYYNGFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "subsetting rows with query"
      ],
      "metadata": {
        "id": "X7XBobLxQxXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge gdp and pop on date and country with fill\n",
        "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
        "\n",
        "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
        "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
        "\n",
        "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
        "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
        "\n",
        "# Select dates equal to or greater than 1991-01-01\n",
        "recent_gdp_pop = gdp_pivot.query('date>=\"1991-01-01\"')\n",
        "\n",
        "# Plot recent_gdp_pop\n",
        "recent_gdp_pop.plot(rot=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CXiMXoD5Qzgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using.melt() to reshape government data"
      ],
      "metadata": {
        "id": "Fe3TmQSbRXtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unpivot everything besides the year column\n",
        "ur_tall = ur_wide.melt(id_vars=['year'], var_name='month', value_name='unempl_rate')\n",
        "\n",
        "\n",
        "# Create a date column using the month and year columns of ur_tall\n",
        "ur_tall['date'] = pd.to_datetime(ur_tall['year'] + '-' + ur_tall['month'])\n",
        "\n",
        "# Sort ur_tall by date in ascending order\n",
        "ur_sorted = ur_tall.sort_values(by='date')\n",
        "\n",
        "# Plot the unempl_rate by date\n",
        "ur_sorted.plot(x='date', y='unempl_rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MvW7fpR8RdOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using.melt() for stock vs bond performance"
      ],
      "metadata": {
        "id": "02HCeW3qRl2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use melt on ten_yr, unpivot everything besides the metric column\n",
        "bond_perc = ten_yr.melt(id_vars=['metric'], var_name='date', value_name='close')\n",
        "\n",
        "# Use query on bond_perc to select only the rows where metric=close\n",
        "bond_perc_close = bond_perc.query('metric==\"close\"')\n",
        "\n",
        "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
        "dow_bond = pd.merge_ordered(dji, bond_perc_close, on='date', how='inner', suffixes=['_dow', '_bond'])\n",
        "\n",
        "\n",
        "# Plot only the close_dow and close_bond columns\n",
        "dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dAsIazc8RqLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}