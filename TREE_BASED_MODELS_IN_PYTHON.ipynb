{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TREE-BASED MODELS IN PYTHON",
      "provenance": [],
      "authorship_tag": "ABX9TyPpQUfzEc4gQM6IPSN5XPPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelRuby/RubytheScientist.github.io/blob/main/TREE_BASED_MODELS_IN_PYTHON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQCbENp7W0AM"
      },
      "outputs": [],
      "source": [
        "TREE BASED-MODELS IN PYTHON\n",
        "\n",
        "Learn how to use tree-based models and ensembles for regression and classification with scikit-learn in python (DataCamp).\n",
        "\n",
        "Classification and Regression Trees\n",
        "Classification and Regression Trees (CART) are a set of supervised learning models used for problems involving classification and regression.\n",
        "\n",
        "Decision-Tree: data structure consisting of a hierarchy of nodes\n",
        "Node: question or prediction\n",
        "Root: no parent node, question giving rise to two children nodes.\n",
        "Internal node: one parent node, question giving rise to two children nodes.\n",
        "Leaf: one parent node, no children nodes –> prediction.\n",
        "\n",
        "\n",
        "Classification Tree\n",
        "Sequence of if-else questions about individual features, and learns patterns from the features in such a way to produce the purest leafs.\n",
        "In the end, in each leaf, one class-label is predominant.\n",
        "Objective: infer class labels\n",
        "Able to capture non-linear relationships between features and labels\n",
        "Don’t require feature scaling (e.g. Standardization, ..)\n",
        "Decision Regions\n",
        "\n",
        "Decision region: region in the feature space where all instances are assigned to one class label.\n",
        "Decision Boundary: surface separating different decision regions.\n",
        "Linear boundary\n",
        "Non-linear boundary\n",
        "# Import DecisionTreeClassifier from sklearn.tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
        "dt = DecisionTreeClassifier(max_depth = 6, random_state = 1)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = dt.predict(X_test)\n",
        "print(y_pred[0:5])\n",
        "\n",
        "# Compute test set accuracy  \n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: {:.2f}\".format(acc))\n",
        "Information Gain\n",
        "How does a classification-Tree learn?\n",
        "\n",
        "The nodes of a classification tree are grown recursively: the obtention of an internal node or a leaf depends on the state of its predecessors.\n",
        "To produce the purest leaves possible, at each node, a tree asks a question involving one feature [Math Processing Error] and a split-point [Math Processing Error].\n",
        "But how does it know which feature and which split-point to pick? It does so by maximizing information gain, i.e. maxmize [Math Processing Error]\n",
        "If it is a unconstrained tree and the [Math Processing Error] = 0, declare the node a leaf.\n",
        "If it is a constrained tree, like the max_depth was set to 2, then it will stop at the set depth no matter the value of the [Math Processing Error].\n",
        "The tree considers that every node contains information and aims at maximizing the information gain obtained after each split.\n",
        "\n",
        "\n",
        "\n",
        "[Math Processing Error]\n",
        "To measure the information gain, we measure the impurity of a node:\n",
        "\n",
        "Criteria to measure the impurity of a node [Math Processing Error]:\n",
        "gini index\n",
        "entropy\n",
        "…\n",
        "Compare accuracy between models trained with entropy and gini index:\n",
        "\n",
        "# Import DecisionTreeClassifier from sklearn.tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
        "dt_entropy = DecisionTreeClassifier(max_depth = 8, criterion = 'entropy', random_state = 1)\n",
        "\n",
        "# Fit dt_entropy to the training set\n",
        "dt_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Import accuracy_score from sklearn.metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Use dt_entropy to predict test set labels\n",
        "y_pred= dt_entropy.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy_entropy\n",
        "accuracy_entropy = accuracy_score(y_pred, y_test)\n",
        "\n",
        "# Print accuracy_entropy\n",
        "print('Accuracy achieved by using entropy: ', accuracy_entropy)\n",
        "\n",
        "# Print accuracy_gini\n",
        "print('Accuracy achieved by using the gini index: ', accuracy_gini)\n",
        "\n",
        "# output:\n",
        "    # Accuracy achieved by using entropy:  0.929824561404\n",
        "    # Accuracy achieved by using the gini index:  0.929824561404\n",
        "Notice how the two models achieve exactly the same accuracy. Most of the time, the gini index and entropy lead to the same results. The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn.\n",
        "\n",
        "Regression Tree\n",
        "Tree-based models help to make nonlinear predictions.\n",
        "\n",
        "When a regression tree is trained on a dataset, the impurity of a node is measured using the mean-squared error of the targets in that node:\n",
        "$$I(node) = \\text{MSE}(node) = \\frac{1}{N_{node}}\\sum\\limits_{i\\in\\text{node}}(y^{(i)} - \\hat{y}{node})^2[Math Processing Error]\\underbrace{\\hat{y}{node}}{\\text{mean target value}} = \\frac{1}{N{node}}\\sum\\limits_{i\\in {node}}y^{(i)}$$\n",
        "The regression tree tries to find the splits that produce leaves where in each leaf the target value are, on average, the closest possible to the mean-value of the labels in that particular leaf.\n",
        "\n",
        "When making predictions, a new instance traverses the tree and reaches a certain leaf, and its target variable ‘y’ is computed as the average of the target variables contained in that leaf.\n",
        "$$\\hat{y}{pred}(leaf) = \\frac{1}{N{leaf}}\\sum\\limits_{i \\in {leaf}}y^{(i)}$$\n",
        "\n",
        "# Import DecisionTreeRegressor from sklearn.tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeRegressor(max_depth = 8,\n",
        "             min_samples_leaf = 0.13, # stopping criteria, speficies the minimum % of dp in training set in each leaf\n",
        "            random_state = 3)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Compute y_pred\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# Compute mse_dt\n",
        "mse_dt = MSE(y_test, y_pred)\n",
        "\n",
        "# Compute rmse_dt\n",
        "rmse_dt = mse_dt**(1/2)\n",
        "\n",
        "# Predict test set labels on linear regression\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Compute mse_lr\n",
        "mse_lr = MSE(y_test, y_pred_lr)\n",
        "\n",
        "# Compute rmse_lr\n",
        "rmse_lr = mse_lr**(1/2)\n",
        "\n",
        "# Print rmse_lr\n",
        "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
        "\n",
        "# Print rmse_dt\n",
        "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))\n",
        "Bias-Variance Tradeoff\n",
        "In supervised learning, we make the assumption that there is a mapping [Math Processing Error] between features and labels.\n",
        "[Math Processing Error]\n",
        "\n",
        "Here, [Math Processing Error] is an unknown function that you want to determine.\n",
        "\n",
        "In reality, data generation is always accompanied with randomness or noises. Therefore, our goal is to find a model [Math Processing Error] that best approximates [Math Processing Error].\n",
        "\n",
        "[Math Processing Error] can be Logistic Regression, Decision Tree, Neural Network…\n",
        "In the process of training the model, we want to discard noises as much as possible, and also hoping to achieve a low predictive error on unseen datasets.\n",
        "\n",
        "Generalization Error\n",
        "Generalization Error of [Math Processing Error]: Does [Math Processing Error] generalize well on unseen data?\n",
        "\n",
        "It can be decomposed as follows:\n",
        "[Math Processing Error]\n",
        "Bias: error term that tells you, on average, how much [Math Processing Error]\n",
        "High bias models lead to underfitting, which means [Math Processing Error] is not flexible enough to approximate [Math Processing Error].\n",
        "When underfitting, the training set error is roughly equal to the test set error, and both errors are relatively high.\n",
        "Variance: error term that tells you how much [Math Processing Error] is inconsistent over different training sets.\n",
        "\n",
        "High variance models lead to overfitting, which means [Math Processing Error] fits the training set noise. Thus it has very low predictive power on unseen data.\n",
        "When overfitting, the testing set error is much much higher than the training set error.\n",
        "Irreducible error: the error contribution of noise, and always a constant.\n",
        "\n",
        "Model Complexity: sets the flexibility of [Math Processing Error] to approximate the true function [Math Processing Error], like setting maximum tree depth, minimum samples per leaf…\n",
        "\n",
        "When the model complexity increases, the variance increases while the bias decreases, and vice versa.\n",
        "The best model complexity corresponds to the lowest generalization error.\n",
        "This is known as the Bias-Variance Tradeoff\n",
        "\n",
        "\n",
        "Evaluate Performance\n",
        "We cannot directly calculate the generalization error of a model because:\n",
        "[Math Processing Error] is unknown\n",
        "usually you only have one dataset\n",
        "noise is unpredictable\n",
        "To estimate the generalization error of a model,\n",
        "\n",
        "split the data to training and test sets,\n",
        "fit [Math Processing Error] to the training set,\n",
        "evaluate the error of [Math Processing Error] on the unseen test set\n",
        "generalization error of [Math Processing Error] test set error of [Math Processing Error]\n",
        "Since test set should not be touched until we are confident about [Math Processing Error]’s performance, and only want to use it to evaluate [Math Processing Error]’s final performance, we introduce cross-validation.\n",
        "\n",
        "K-Fold CV: [Math Processing Error]\n",
        "Hold-Out CV\n",
        "\n",
        "\n",
        "Diagosis:\n",
        "\n",
        "If CV error of [Math Processing Error] > training set error of [Math Processing Error]:\n",
        "[Math Processing Error] suffers from high variance and is overfitting.\n",
        "Solutions:\n",
        "Decrease model complexity (decrease max depth, increase min samples per leaf…)\n",
        "Gather more data\n",
        "If CV error of [Math Processing Error] training set error of [Math Processing Error] desired error:\n",
        "[Math Processing Error] suffers from high bias and is underfitting\n",
        "Solutions:\n",
        "Increase model complexity (increase max depth, decrease min samples per leaf…)\n",
        "Gather more relevant features\n",
        "# Import train_test_split from sklearn.model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set SEED for reproducibility\n",
        "SEED = 1\n",
        "\n",
        "# Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED)\n",
        "\n",
        "# Instantiate a DecisionTreeRegressor dt\n",
        "dt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.26, random_state = SEED)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "## Note that since cross_val_score has only the option of evaluating the negative MSEs, \n",
        "## its output should be multiplied by negative one to obtain the MSEs.\n",
        "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv = 10, \n",
        "                       scoring = 'neg_mean_squared_error',\n",
        "                       n_jobs=-1) # all cpus are used for calculation\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
        "# CV RMSE: 5.14\n",
        "\n",
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the training set\n",
        "y_pred_train = dt.predict(X_train)\n",
        "\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
        "## output:\n",
        "    # Train RMSE: 5.15\n",
        "Ensemble Learning\n",
        "CART’s advantages\n",
        "Simple to understand and interpret\n",
        "Easy to use\n",
        "Flexibility: ability to describe non-linear dependencies\n",
        "Simple preprocessing: no need to standardize or normalize features\n",
        "CART’s limitation\n",
        "Classification can only produce orthogonal decision boundaries (rectangular)\n",
        "Sensitive to small variations in the training set\n",
        "High variance: unconstrained CARTs may overfit the training set\n",
        "Solution: ensemble learning\n",
        "Basics of Emsemble Learning\n",
        "\n",
        "Train different models on the same dataset.\n",
        "Let each model make its prediction.\n",
        "Meta-model: aggregates predictions ofindividual models.\n",
        "Final prediction: more robust and less prone to errors.\n",
        "Best results: models are skillful in different ways.\n",
        "Voting Classifier\n",
        "EXAMPLE: Binary classfication task\n",
        "\n",
        "[Math Processing Error] classifiers make predictions: [Math Processing Error] with [Math Processing Error] or [Math Processing Error]\n",
        "Meta-model prediction: hard voting (majority vote)\n",
        "\n",
        "# Import functions to compute accuracy and split data\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import models, including VotingClassifier meta-model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 1\n",
        "\n",
        "# Instantiate lr\n",
        "lr = LogisticRegression(random_state = SEED)\n",
        "\n",
        "# Instantiate knn\n",
        "knn = KNN(n_neighbors = 27)\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(min_samples_leaf = 0.13, random_state = SEED)\n",
        "\n",
        "# Define the list classifiers\n",
        "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
        "\n",
        "# Iterate over the pre-defined list of classifiers\n",
        "for clf_name, clf in classifiers:    \n",
        " \n",
        "    # Fit clf to the training set\n",
        "    clf.fit(X_train, y_train)    \n",
        "   \n",
        "    # Predict y_pred\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_pred, y_test) \n",
        "   \n",
        "    # Evaluate clf's accuracy on the test set\n",
        "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
        "\n",
        "# output:\n",
        "#     Logistic Regression : 0.747\n",
        "#     K Nearest Neighbours : 0.724\n",
        "#     Classification Tree : 0.730\n",
        "\n",
        "# Import VotingClassifier from sklearn.ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Instantiate a VotingClassifier vc\n",
        "vc = VotingClassifier(estimators = classifiers)     \n",
        "\n",
        "# Fit vc to the training set\n",
        "vc.fit(X_train, y_train)   \n",
        "\n",
        "# Evaluate the test set predictions\n",
        "y_pred = vc.predict(X_test)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print('Voting Classifier: {:.3f}'.format(accuracy))\n",
        "\n",
        "# output:\n",
        "#     Voting Classifier: 0.753\n",
        "Bagging and Random Forests\n",
        "Bagging\n",
        "Bagging: Bootstrap Aggregation\n",
        "\n",
        "\n",
        "Uses a techinque known as bootstrap (sample with replacement for multiple times at a fixed size)\n",
        "Reduces variance of individual models in the ensemble.\n",
        "Train each model on a bootstrap subset of the traning set\n",
        "Output a final prediction:\n",
        "Classification: aggregates predictions by majority voting. BaggingClassifier\n",
        "Regression: aggregates predictions through averaging. BaggingRegressor\n",
        "# Import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import BaggingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(random_state = 1)\n",
        "\n",
        "# Instantiate bc\n",
        "bc = BaggingClassifier(\n",
        "    base_estimator = dt, \n",
        "    n_estimators = 50, # number of bootstrap samples\n",
        "    random_state = 1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fit bc to the training set\n",
        "bc.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = bc.predict(X_test)\n",
        "\n",
        "# Evaluate acc_test\n",
        "acc_test = accuracy_score(y_pred, y_test)\n",
        "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) \n",
        "\n",
        "# output:\n",
        "#     Test set accuracy of bc: 0.71\n",
        "# A single tree dt would have achieved an accuracy of 63% \n",
        "# which is 8% lower than bc's accuracy\n",
        "Out-of-Bag Evaluation\n",
        "When performing a bootstrapping, there will be a lot of instances that have not been sampled. Therefore, we can use these unused data points to test the performance of the model, instead of using CV.\n",
        "\n",
        "\n",
        "\n",
        "[Math Processing Error]\n",
        "In scikit-learn, OOB score corresponds to the accuracy of classifiers, while r-squared score are for regressors.\n",
        "\n",
        "# Import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import BaggingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(min_samples_leaf = 8, random_state = 1)\n",
        "\n",
        "# Instantiate bc\n",
        "bc = BaggingClassifier(base_estimator = dt, \n",
        "            n_estimators = 50,\n",
        "            oob_score = True,\n",
        "            random_state = 1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fit bc to the training set \n",
        "bc.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = bc.predict(X_test)\n",
        "\n",
        "# Evaluate test set accuracy\n",
        "acc_test = accuracy_score(y_pred, y_test)\n",
        "\n",
        "# Evaluate OOB accuracy\n",
        "acc_oob = bc.oob_score_\n",
        "\n",
        "# Print acc_test and acc_oob\n",
        "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))\n",
        "\n",
        "# output:\n",
        "#     Test set accuracy: 0.698, OOB accuracy: 0.704\n",
        "Random Forest\n",
        "Recall we talked above that for Bagging:\n",
        "\n",
        "Base estimator can be: Decision Tree, LogisticRegression, Neural Net, …\n",
        "Each estimator is trained on a distinct bootstrap sample of the training set\n",
        "Estimators use all features for training and prediction\n",
        "Random Forest:\n",
        "\n",
        "is an ensemble method\n",
        "Base estimator: Decision Tree\n",
        "Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
        "RF introduces further randomization in the training of individual trees\n",
        "[Math Processing Error] features are sampled at each node without replacement ([Math Processing Error] < total number of features [Math Processing Error])\n",
        "The default value of [Math Processing Error] is [Math Processing Error], where [Math Processing Error] stands for the total number of features.\n",
        "Output a final prediction:\n",
        "Classification: aggregates predictions by majority voting. RandomForestClassifier\n",
        "Regression: aggregates predictions through averaging. RandomForestRegressor\n",
        "\n",
        "\n",
        "In general, random forest achieves a lower variance than an individual tree\n",
        "\n",
        "Feature Importance\n",
        "\n",
        "Tree-based methods enable measuring the importance of each feature in prediction.\n",
        "Intuitively, it means how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
        "Can be accessed using the attribute feature_importance_\n",
        "It is expressed as a percentage indicating the weight of that feature in training and prediction\n",
        "Visualization of feature importances:\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a pd.Series of features importances\n",
        "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
        "\n",
        "# Sort importances_rf\n",
        "sorted_importances_rf = importances_rf.sort_values()\n",
        "\n",
        "# Make a horizontal bar plot\n",
        "sorted_importances_rf.plot(kind = 'barh', color = 'lightgreen')\n",
        "plt.show()\n",
        "Performing RF regression\n",
        "\n",
        "# Import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Instantiate rf\n",
        "rf = RandomForestRegressor(n_estimators = 25,\n",
        "            random_state = 2)\n",
        "            \n",
        "# Fit rf to the training set    \n",
        "rf.fit(X_train, y_train) \n",
        "\n",
        "# Import mean_squared_error as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Predict the test set labels\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the test set RMSE\n",
        "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
        "\n",
        "# Print rmse_test\n",
        "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
        "\n",
        "#output:\n",
        "    # Test set RMSE of rf: 51.97\n",
        "Boosting\n",
        "Boosting refers to an ensemble method in which many predictors are trained and each predictor learns from the errors of its predecessor.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Ensemble method combining several weak learners to from a strong learner\n",
        "Weak learner: Model doing slightly better than radom guessing\n",
        "e.g. Decision stump (CART whose max depth is 1)\n",
        "Train predictors sequentially\n",
        "Each predictor tries to correct its predecessor\n",
        "AdaBoost\n",
        "Stands for Adaptive Boosting\n",
        "Each predictor pays more attention to the instances wrongly predicted by its predecessor, which is achieved by chaging the weights of training instances after each individual model.\n",
        "Each predictor is assigne d a coefficient [Math Processing Error], which depends on the predictor’s training error\n",
        "Details:\n",
        "Suppose we have [Math Processing Error] predictors in total.\n",
        "The [Math Processing Error] is trained on the initial dataset [Math Processing Error], and the training error [Math Processing Error] is determined.\n",
        "We use [Math Processing Error] to determine [Math Processing Error], which is [Math Processing Error]’s coefficient.\n",
        "[Math Processing Error] is then used to determine [Math Processing Error] of the training instances for [Math Processing Error].\n",
        "Here, the incorrectly predicted instances will gain a higher weight, which would force the next predictor to pay more attention to these instances.\n",
        "The above process is repeated sequentially, until [Math Processing Error] predictors forming the ensemble are trained.\n",
        "An important parameter: learning rate [Math Processing Error]\n",
        "[Math Processing Error]\n",
        "It is used to shrink the coefficient [Math Processing Error] of a trained predictor\n",
        "Tradeoff between [Math Processing Error] and the number of predictors\n",
        "Small [Math Processing Error] should be compensated by a greater number of predictors.\n",
        "Output a final prediction:\n",
        "Classification: aggregates predictions by majority voting. AdaBoostClassifier\n",
        "Regression: aggregates predictions through averaging. AdaBoostRegressor\n",
        "\n",
        "\n",
        "# Import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import AdaBoostClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(max_depth = 2, random_state=1)\n",
        "\n",
        "# Instantiate ada\n",
        "ada = AdaBoostClassifier(base_estimator = dt, n_estimators = 180, random_state = 1)\n",
        "\n",
        "# Fit ada to the training set\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Compute the probabilities of obtaining the positive class\n",
        "y_pred_proba = ada.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Import roc_auc_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Evaluate test-set roc_auc_score\n",
        "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print roc_auc_score\n",
        "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n",
        "\n",
        "#output:\n",
        "    # ROC AUC score: 0.71\n",
        "Gradient Boosting\n",
        "Sequential correction of predecessor’s errors.\n",
        "Does not tweak the weights of training instances.\n",
        "Fit each predictor is trained using its predecessor’s residual errors as labels.\n",
        "Gradient Boosted Trees: a CART is used as a base learner.\n",
        "Gradient Boosted Trees for Regression:\n",
        "\n",
        "Details:\n",
        "The ensemble consists [Math Processing Error] trees\n",
        "[Math Processing Error] is trained using feature matrix [Math Processing Error] and the dataset labels [Math Processing Error]\n",
        "The prediction [Math Processing Error] are used to determine the training set residual errors [Math Processing Error]\n",
        "[Math Processing Error] is then trained using feature matrix [Math Processing Error] and residual error [Math Processing Error] as labels.\n",
        "The predicted residuals times learning rate, [Math Processing Error], is then used to determine the residuals of residuals, which are labeled [Math Processing Error]\n",
        "This process is repeated until all of the [Math Processing Error] trees forming the ensemble are trained.\n",
        "Important parameter: learning rate [Math Processing Error]\n",
        "[Math Processing Error]\n",
        "It is used to shrink the labels of each tree, which is essentially [Math Processing Error] of the previous trained tree\n",
        "Tradeoff between [Math Processing Error] and the number of predictors\n",
        "Small [Math Processing Error] should be compensated by a greater number of predictors.\n",
        "Output a final prediction:\n",
        "Classification:\n",
        "not covered in this course\n",
        "GradientBoostingClassifier\n",
        "Regression:\n",
        "[Math Processing Error]\n",
        "GradientBoostingRegressor\n",
        "\n",
        "\n",
        "# Import GradientBoostingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Instantiate gb\n",
        "gb = GradientBoostingRegressor(max_depth = 4, \n",
        "            n_estimators = 200,\n",
        "            random_state = 2)\n",
        "\n",
        "# Fit gb to the training set\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Import mean_squared_error as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Compute MSE\n",
        "mse_test = MSE(y_test, y_pred)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse_test = mse_test**(1/2)\n",
        "\n",
        "# Print RMSE\n",
        "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))\n",
        "\n",
        "#output:\n",
        "    # Test set RMSE of gb: 52.065\n",
        "Stochastic Gradient Boosting\n",
        "Gradient Boosting has its limitations:\n",
        "\n",
        "GB involves an exhaustive search procedure.\n",
        "Each CART is trained to find the best split points and features, and thus may lead to CARTs using the same split points and maybe the same features.\n",
        "Stochastic Gradient Boosting helps avoid this from happening:\n",
        "\n",
        "Each tree is trained on a random subset of rows of the training data.\n",
        "The sampled instances (40%-80% of the training set) are sampled without replacement.\n",
        "Features are sampled (without replacement) when choosing split points.\n",
        "Result: further ensemble diversity.\n",
        "Effect: adding further variance to the ensemble of trees.\n",
        "Details of SGB:\n",
        "\n",
        "We randomly sample only a fraction of the training set without replacement to feed into the first predictor [Math Processing Error].\n",
        "Then, when it comes to features, we ramdomly sample a fraction of the features without replacement when considering makeing a split.\n",
        "Once a tree is trained, predictions ([Math Processing Error]) are made and the residual errors [Math Processing Error] on the training set can be calculated.\n",
        "These residual errors are then multiplied by learning rate [Math Processing Error] and fed to the next tree in the ensemble.\n",
        "This procedure is repeated sequentially until all the trees in the ensemble are trained.\n",
        "The prediction procedure of SGB is similar to GB.\n",
        "\n",
        "\n",
        "# Import GradientBoostingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Instantiate sgbr\n",
        "sgbr = GradientBoostingRegressor(\n",
        "    max_depth = 4, \n",
        "    subsample = 0.9, # set the fraction of training set to use in each tree\n",
        "    max_features = 0.75, # set the fraction of features to use when making a split\n",
        "    n_estimators = 200,\n",
        "    random_state = 2)\n",
        "\n",
        "# Fit sgbr to the training set\n",
        "sgbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = sgbr.predict(X_test)\n",
        "\n",
        "# Import mean_squared_error as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Compute test set MSE\n",
        "mse_test = MSE(y_pred, y_test)\n",
        "\n",
        "# Compute test set RMSE\n",
        "rmse_test = mse_test**(1/2)\n",
        "\n",
        "# Print rmse_test\n",
        "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))\n",
        "# output:\n",
        "    # Test set RMSE of sgbr: 49.979\n",
        "\n",
        "# The stochastic gradient boosting regressor achieves \n",
        "# a lower test set RMSE than the gradient boosting regressor \n",
        "# (which was 52.065)\n",
        "Model Tuning\n",
        "Parameters: learned from data\n",
        "CART example: split-point of a node, split-feature of a node, …\n",
        "Hyperparameters: not learned from data, set prior to training\n",
        "CART example: max_depth , min_samples_leaf , splitting criterion …\n",
        "Hyperparameter Tuning\n",
        "\n",
        "Problem: search for a set of optimal hyperparameters for a learning algorithm.\n",
        "Solution: find a set of optimal hyperparameters that results in an optimal model, which yields an optimal score.\n",
        "Score: in sklearn defaults to accuracy (classication) and R (regression).\n",
        "Cross validation is used to estimate the generalization performance.\n",
        "Possible approaches:\n",
        "Grid Search\n",
        "Random Search\n",
        "Bayesian Optimization\n",
        "Genetic Algorithms\n",
        "…\n",
        "Grid search cross validation\n",
        "\n",
        "Manually set a grid of discrete hyperparameter values.\n",
        "Set a metric for scoring model performance.\n",
        "Search exhaustively through the grid.\n",
        "For each set of hyperparameters, evaluate each model’s CV score.\n",
        "The optimal hyperparameters are those ofthe model achieving the best CV score.\n",
        "It suffers from the curse of dimensionality\n",
        "it is computationally expensive\n",
        "and sometimes lead to very slight improvement\n",
        "We should always weigh the impact of tuning in the whole project!\n",
        "Using model.get_params() helps you know what are the hyperparameters behind this model.\n",
        "\n",
        "If we set refit to True in grid search, the best model in the end will already be trained on the entire training set.\n",
        "\n",
        "verbose controls verbosity. The higher the value, the more messages are outputted.\n",
        "\n",
        "Tuning CART\n",
        "# Define params_dt\n",
        "params_dt = {\n",
        "    'max_depth':[2, 3, 4],\n",
        "    'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]\n",
        "}\n",
        "\n",
        "# Import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Instantiate grid_dt\n",
        "grid_dt = GridSearchCV(estimator = dt,\n",
        "                       param_grid = params_dt,\n",
        "                       scoring = 'roc_auc',\n",
        "                       cv = 5,\n",
        "                       n_jobs = -1)\n",
        "\n",
        "# Import roc_auc_score from sklearn.metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Extract the best estimator\n",
        "best_model = grid_dt.best_estimator_\n",
        "\n",
        "# Predict the test set probabilities of the positive class\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute test_roc_auc\n",
        "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print test_roc_auc\n",
        "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))\n",
        "\n",
        "#output:\n",
        "    # Test set ROC AUC score: 0.610\n",
        "# An untuned classification-tree would achieve a ROC AUC score of 0.54\n",
        "Tuning Random Forest\n",
        "# Define the dictionary 'params_rf'\n",
        "params_rf = {\n",
        "    'n_estimators':[100, 350, 500],\n",
        "    'max_features':['log2', 'auto', 'sqrt'],\n",
        "    'min_samples_leaf':[2, 10, 30]\n",
        "}\n",
        "\n",
        "# Import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Instantiate grid_rf\n",
        "grid_rf = GridSearchCV(estimator = rf,\n",
        "                       param_grid = params_rf,\n",
        "                       scoring = 'neg_mean_squared_error',\n",
        "                       cv = 3,\n",
        "                       verbose = 1,\n",
        "                       n_jobs = -1)\n",
        "\n",
        "# Import mean_squared_error from sklearn.metrics as MSE \n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Extract the best estimator\n",
        "best_model = grid_rf.best_estimator_\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Compute rmse_test\n",
        "rmse_test = MSE(y_pred, y_test)**(1/2)\n",
        "\n",
        "# Print rmse_test\n",
        "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) \n",
        "\n",
        "#output:\n",
        "    # Test RMSE of best model: 50.569"
      ]
    }
  ]
}